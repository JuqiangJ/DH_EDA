[
["index.html", "A Introduction to Eploratory Data Analysis with R Chapter 1 Prerequisites 1.1 Installing R and R studio 1.2 Install packages and library packages 1.3 Data and workbook", " A Introduction to Eploratory Data Analysis with R Juqiang Chen 2019-12-01 Chapter 1 Prerequisites This document is to accompany Eploratory Data Analysis with R tutorial for DH Downunder 2019 at the University of Newcastle, Australia, from 9-13 December. I am a speech scientist working on cross-language lexical tone perception and production. I have rich experience dealing with experimental data and I am keen to help others with data wrangling, data visualization and statistical modelling problems. I aspire to promote a streamlined workflow with R packages to improve data analysis efficiency in quantitative analysis in the field of social science and linguistics. If you have any questions about the tutorial, please e-mail me at: j.chen2@westernsydney.edu.au This workshop will show how to use data transformation and visualization to explore your data in a systematic way, or in a statistical term, exploratory data analysis. Participants will learn to generate questions about the data, search for answers by transforming, visualizing and modeling the dataset, and use what they learn to further refine the questions and/or generate new questions. The workshop will start by exploring variations in (categorical and continuous) one variable and move on to investigate covariations among two or three variables. Participants will learn to produce summary tables (calculating mean or standard deviation etc. of one or multiple variables by one or more variables) and will also learn to draw figures with ggplot2. This workshop builds on some knowledge of data wrangling. Therefore, it is desirable that participants should take the Introduction to data wrangling with R, if they have no such knowledge. Participants are welcomed to bring their own data and apply what they learn on the spot. Before we start our journey of data wrangling with R, you will need to install R on your laptop. R is multi-platform, which means you can install R on your PC or MAC. 1.1 Installing R and R studio Use this link [https://cloud.r-project.org/] to download R and select the proper version for your laptop. knitr::include_graphics(&quot;img/installr.jpg&quot;) Figure 1.1: Download R RStudio is an integrated development environment, or IDE, for R programming. Download and install it from [http://www.rstudio.com/download.] The free version is poweful enough. 1.2 Install packages and library packages Packages are collections of R functions, data, and compiled code in a well-defined format. The directory where packages are stored is called the library. R comes with a standard set of packages. Others are available for download and installation. install.packages(“package_name”) Please type in: install.packages(“tidyverse”) tidyverse is a set of R packages for data science in R and it takes a few minutes to download. Once installed, they have to be loaded into the session to be used. library(package_name) Please type in library(tidyverse) 1.3 Data and workbook Please download the data and workbook with this link. https://drive.google.com/open?id=18xwfD7f-eekQ60W-2IZ2wBkCX2GL08_u "],
["intro.html", "Chapter 2 Basic data structures in R 2.1 Types of variables: A taxonomy 2.2 1D data structure: vectors 2.3 2D data structures: matrice and data frames 2.4 Summary", " Chapter 2 Basic data structures in R Before we jump into actual data analysis, it is desirable to first think about what are common variable types and how they are stored in R. A variable is any characteristics, number, or quantity that can be measured or counted. A variable may also be called a data item. Age, sex, business income and expenses, country of birth, capital expenditure, class grades, eye colour and vehicle type are examples of variables. It is called a variable because the value may vary between data units in a population, and may change in value over time.(Australian Bureau of Statistics, ABS) 2.1 Types of variables: A taxonomy 2.1.1 Categorical variables: ordinal vs. norminal Categorical variables have values that describe a ‘quality’ or ‘type’ or ‘category’.Therefore, categorical variables are qualitative variables and tend to be represented by a non-numeric value. Categorical variables should be exclusive (in one category or in another) and exhaustive (include all possible options). Categorical variables may be further divided as being ordinal or nominal: An ordinal variable can be logically ordered or ranked. The categories associated with ordinal variables can be ranked higher or lower than another, but do not necessarily establish a numeric difference between each category.In other words, the interval between levels of the variables are unknown. Examples of ordinal categorical variables include academic grades (i.e. A, B, C), clothing size (i.e. small, medium, large, extra large) and attitudes (i.e. strongly agree, agree, disagree, strongly disagree). For example, when doing a survey, participants will be asked to rate. The subjective measurements of this kind are often ordinal variables. E.g. a Likert ranking scale; level of education (“&lt; high school”, “high school”, “associate’s degree”). We can assign numbers to different levels of an ordinal variable, but we should bear in mind that these variable are not numeric. For example, “strongly agree” and “neutral” cannot average out to an “agree”, even though you may assign 5 to “strong agree” and 3 to “neutral”. A nominal variable is not able to be organised in a logical sequence. Examples of nominal categorical variables include sex, business type, eye colour, religion and brand. The data collected for a categorical variable are qualitative data. 2.1.2 Numeric variables: discrete or continuous Numeric variables have values that describe a measurable quantity as a number, like ‘how many’ or ‘how much’. Therefore, numeric variables are quantitative variables.(ABS) It is also called Interval/ratio variables and the interval between numbers is equal: the interval between 1 kg and 2 kg is the same as between 3 kg and 4 kg. Numeric variables may be further divided as being either continuous or discrete: A discrete variable consists of counts from a set of distinct whole values. A discrete variable cannot take the value of a fraction between one value and the next closest value. Examples of discrete variables include the number of registered cars, number of business locations, and number of children in a family, all of of which measured as whole units (i.e. 1, 2, 3 cars).(ABS) A continuous variable can take any value between a certain set of real numbers. The value given to an observation for a continuous variable can include values as small as the instrument of measurement allows. Examples of continuous variables include height, time, age, and temperature.(ABS) The data collected for a numeric variable are quantitative data. The variable type will determine (1) statistical analysis; (2) the way we summarize data with statistics and plots. (#fig:variable_type)Taxonomy of variables Variables can be stored in R in different data types. Normial and ordinal variables can be stored as character or factors (with levels). Interval data are stored as numbers either as integer or numeric (real or decimal). If you have only one variable, you can store it in a vector. However, more often than not, you have a bunch of variables that need to be stored or imported as a matrix or data frame. 2.2 1D data structure: vectors A vector is a sequence of data elements of the same basic type: integer, double, logical or character. All elements of a vector must be the same type. 2.2.1 Creating vectors a = 8:17 b &lt;- c(9, 10, 100, 38) c = c (TRUE, FALSE, TRUE, FALSE) c = c (T, F, T, F) d = c (&quot;TRUE&quot;, &quot;FALSE&quot;, &quot;FALSE&quot;) # You can change the type of a vector with as.vector function. as.vector(b, mode = &quot;character&quot;) ## [1] &quot;9&quot; &quot;10&quot; &quot;100&quot; &quot;38&quot; # When you put elements of different types in one vector, R will automatically change the type of some elements to keep the whole vector homogenous. e = c(9,10, &quot;ab&quot;, &quot;cd&quot;) f = c(10, 11, T, F) c () is a function in R. There are some other basic functions in R that you can play with to generate vectors. A = 9:20 + 1 B = seq (1, 10) C = seq (1, 20, by= 2) D = rep (5, 4) E = rep (c(1,2,3), 4) G = rep (c(1,2,3), each = 4) # Now that you have a vector, you can do some Maths. max(a) ## [1] 17 min(a) ## [1] 8 range(a) ## [1] 8 17 sum(a) ## [1] 125 mean(a) ## [1] 12.5 median(a) ## [1] 12.5 quantile(a) ## 0% 25% 50% 75% 100% ## 8.00 10.25 12.50 14.75 17.00 sd(a) ## [1] 3.02765 round(sd(a), 2) ## [1] 3.03 2.2.2 Creating list objects We can put vectors of different types (e.g., number, logic or character) and lengths in a list object. list1 = list(a, b, c, d, e, f) list1 ## [[1]] ## [1] 8 9 10 11 12 13 14 15 16 17 ## ## [[2]] ## [1] 9 10 100 38 ## ## [[3]] ## [1] TRUE FALSE TRUE FALSE ## ## [[4]] ## [1] &quot;TRUE&quot; &quot;FALSE&quot; &quot;FALSE&quot; ## ## [[5]] ## [1] &quot;9&quot; &quot;10&quot; &quot;ab&quot; &quot;cd&quot; ## ## [[6]] ## [1] 10 11 1 0 # More often than not, we do not make list ourselves but have to deal with lists when we get outputs from stats models. 2.3 2D data structures: matrice and data frames Most of us have had some experience with the Excel spreadsheet. Data in a spreadsheet are arranged by rows and columns in a rectangular space. This is a typical 2 dimensional data structure. In R, we can have two ways of forming tabular data like a spreadsheet: the matrix and dataframe. A matrix is a collection of data elements arranged in a two-dimensional rectangular layout in which all the elements must be of the same type (e.g., numeric or character). Dataframe is similar to matrix in shape but only differs in that different types of data (e.g. numeric, factor, character) can co-exist in different columns. Thus, in data analysis, we use dataframes more often than matrix. # Let&#39;s generate a dataframe from scratch. id = seq(1, 40) gender = rep(c(&quot;male&quot;, &quot;female&quot;), 5) maths = rnorm(40, mean = 70, sd = 5) english = rnorm(40, mean = 80, sd = 9) music = rnorm(40, mean = 75, sd = 10) pe = rnorm(40, mean = 86, sd = 12) df1 = data.frame (id, gender, maths, english) Now let’s explore the data frame we just created. str(df1) ## &#39;data.frame&#39;: 40 obs. of 4 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ gender : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 1 2 1 2 1 2 1 ... ## $ maths : num 72.5 72.8 73.1 62.2 73.9 ... ## $ english: num 74.5 82.9 83.1 89.2 84.9 ... summary(df1) ## id gender maths english ## Min. : 1.00 female:20 Min. :60.70 Min. :67.58 ## 1st Qu.:10.75 male :20 1st Qu.:68.71 1st Qu.:74.99 ## Median :20.50 Median :72.00 Median :81.16 ## Mean :20.50 Mean :71.09 Mean :80.76 ## 3rd Qu.:30.25 3rd Qu.:73.70 3rd Qu.:84.93 ## Max. :40.00 Max. :82.42 Max. :97.86 nrow(df1) ## [1] 40 ncol(df1) ## [1] 4 attributes(df1) ## $names ## [1] &quot;id&quot; &quot;gender&quot; &quot;maths&quot; &quot;english&quot; ## ## $class ## [1] &quot;data.frame&quot; ## ## $row.names ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## [24] 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 2.3.1 what if I want to change column names or add variable to the df? df2 = data.frame (id = id, gender = gender, maths = maths, english = english) df2 = cbind(df2, pe) colnames(df2) = c(&quot;ID&quot;, &quot;SEX&quot;,&quot;MATHS&quot;,&quot;ENGLISH&quot;,&quot;PE&quot;) head(df2) ## ID SEX MATHS ENGLISH PE ## 1 1 male 72.53281 74.48894 97.66979 ## 2 2 female 72.75700 82.89130 112.92322 ## 3 3 male 73.14795 83.10436 78.02687 ## 4 4 female 62.21088 89.18430 84.74257 ## 5 5 male 73.88235 84.87322 86.64560 ## 6 6 female 69.53656 72.86251 109.71889 2.3.2 Subsetting dataframes We all know how to select part of an Excel spreadsheet by clicking and moving our mouse. In R, when we want to select part of a dataframe, we use this formula, dataframe[row, column]. There are various ways we can use this formula and believe it or not, you will love them! # the complete dataset df2 ## ID SEX MATHS ENGLISH PE ## 1 1 male 72.53281 74.48894 97.66979 ## 2 2 female 72.75700 82.89130 112.92322 ## 3 3 male 73.14795 83.10436 78.02687 ## 4 4 female 62.21088 89.18430 84.74257 ## 5 5 male 73.88235 84.87322 86.64560 ## 6 6 female 69.53656 72.86251 109.71889 ## 7 7 male 69.69414 90.03711 66.25634 ## 8 8 female 66.77217 81.81902 105.93009 ## 9 9 male 70.36097 88.03261 82.45794 ## 10 10 female 71.10885 89.78673 85.49517 ## 11 11 male 73.44248 78.66106 100.55248 ## 12 12 female 63.98846 72.79863 78.83413 ## 13 13 male 72.81422 73.87937 105.23465 ## 14 14 female 72.24254 78.40693 67.37368 ## 15 15 male 60.83429 80.33713 63.27331 ## 16 16 female 77.12162 73.53552 87.36070 ## 17 17 male 72.65411 83.48973 89.35975 ## 18 18 female 60.70186 80.49504 93.81088 ## 19 19 male 73.64119 73.44537 54.59337 ## 20 20 female 78.35413 80.05504 80.05833 ## 21 21 male 79.57391 72.67337 74.52876 ## 22 22 female 67.56422 87.86359 72.70417 ## 23 23 male 76.30197 75.63349 88.43091 ## 24 24 female 75.90386 68.85129 88.21545 ## 25 25 male 74.74434 80.14026 67.79986 ## 26 26 female 64.92507 75.60258 95.95337 ## 27 27 male 72.01760 82.15092 93.30031 ## 28 28 female 69.09663 85.10584 104.32410 ## 29 29 male 63.30543 70.82359 104.70460 ## 30 30 female 74.30931 67.57838 96.45253 ## 31 31 male 74.76018 94.94564 94.53132 ## 32 32 female 73.09056 97.85612 93.02721 ## 33 33 male 82.42443 76.28375 84.93208 ## 34 34 female 69.99505 83.42918 91.08647 ## 35 35 male 71.86658 82.62143 85.40697 ## 36 36 female 71.72867 84.07834 74.51312 ## 37 37 male 63.14827 85.26186 95.15127 ## 38 38 female 71.83150 81.83022 88.45050 ## 39 39 male 67.16056 90.19692 99.04903 ## 40 40 female 71.98204 75.15054 84.91981 df2[2:5, ] # from row 2 to row 5 ## ID SEX MATHS ENGLISH PE ## 2 2 female 72.75700 82.89130 112.92322 ## 3 3 male 73.14795 83.10436 78.02687 ## 4 4 female 62.21088 89.18430 84.74257 ## 5 5 male 73.88235 84.87322 86.64560 df2[ , 1:2] # select column 1 to 2 ## ID SEX ## 1 1 male ## 2 2 female ## 3 3 male ## 4 4 female ## 5 5 male ## 6 6 female ## 7 7 male ## 8 8 female ## 9 9 male ## 10 10 female ## 11 11 male ## 12 12 female ## 13 13 male ## 14 14 female ## 15 15 male ## 16 16 female ## 17 17 male ## 18 18 female ## 19 19 male ## 20 20 female ## 21 21 male ## 22 22 female ## 23 23 male ## 24 24 female ## 25 25 male ## 26 26 female ## 27 27 male ## 28 28 female ## 29 29 male ## 30 30 female ## 31 31 male ## 32 32 female ## 33 33 male ## 34 34 female ## 35 35 male ## 36 36 female ## 37 37 male ## 38 38 female ## 39 39 male ## 40 40 female df2[ , c(&quot;ENGLISH&quot;, &quot;PE&quot;)] # select by column names ## ENGLISH PE ## 1 74.48894 97.66979 ## 2 82.89130 112.92322 ## 3 83.10436 78.02687 ## 4 89.18430 84.74257 ## 5 84.87322 86.64560 ## 6 72.86251 109.71889 ## 7 90.03711 66.25634 ## 8 81.81902 105.93009 ## 9 88.03261 82.45794 ## 10 89.78673 85.49517 ## 11 78.66106 100.55248 ## 12 72.79863 78.83413 ## 13 73.87937 105.23465 ## 14 78.40693 67.37368 ## 15 80.33713 63.27331 ## 16 73.53552 87.36070 ## 17 83.48973 89.35975 ## 18 80.49504 93.81088 ## 19 73.44537 54.59337 ## 20 80.05504 80.05833 ## 21 72.67337 74.52876 ## 22 87.86359 72.70417 ## 23 75.63349 88.43091 ## 24 68.85129 88.21545 ## 25 80.14026 67.79986 ## 26 75.60258 95.95337 ## 27 82.15092 93.30031 ## 28 85.10584 104.32410 ## 29 70.82359 104.70460 ## 30 67.57838 96.45253 ## 31 94.94564 94.53132 ## 32 97.85612 93.02721 ## 33 76.28375 84.93208 ## 34 83.42918 91.08647 ## 35 82.62143 85.40697 ## 36 84.07834 74.51312 ## 37 85.26186 95.15127 ## 38 81.83022 88.45050 ## 39 90.19692 99.04903 ## 40 75.15054 84.91981 df2[c(1,2,3), ] #select the first three rows ## ID SEX MATHS ENGLISH PE ## 1 1 male 72.53281 74.48894 97.66979 ## 2 2 female 72.75700 82.89130 112.92322 ## 3 3 male 73.14795 83.10436 78.02687 df2[seq(1, 40, 2), ] #select every other rows from 1 to 40 rows ## ID SEX MATHS ENGLISH PE ## 1 1 male 72.53281 74.48894 97.66979 ## 3 3 male 73.14795 83.10436 78.02687 ## 5 5 male 73.88235 84.87322 86.64560 ## 7 7 male 69.69414 90.03711 66.25634 ## 9 9 male 70.36097 88.03261 82.45794 ## 11 11 male 73.44248 78.66106 100.55248 ## 13 13 male 72.81422 73.87937 105.23465 ## 15 15 male 60.83429 80.33713 63.27331 ## 17 17 male 72.65411 83.48973 89.35975 ## 19 19 male 73.64119 73.44537 54.59337 ## 21 21 male 79.57391 72.67337 74.52876 ## 23 23 male 76.30197 75.63349 88.43091 ## 25 25 male 74.74434 80.14026 67.79986 ## 27 27 male 72.01760 82.15092 93.30031 ## 29 29 male 63.30543 70.82359 104.70460 ## 31 31 male 74.76018 94.94564 94.53132 ## 33 33 male 82.42443 76.28375 84.93208 ## 35 35 male 71.86658 82.62143 85.40697 ## 37 37 male 63.14827 85.26186 95.15127 ## 39 39 male 67.16056 90.19692 99.04903 2.4 Summary Dimensions Homogenous Heterogeneous 1D Atomic Vector List 2D Matrix Data frame nD Array "],
["what-is-eda.html", "Chapter 3 What is EDA? 3.1 Definition (from Wikipedia) 3.2 The objectives of EDA 3.3 Data analysis workflow", " Chapter 3 What is EDA? 3.1 Definition (from Wikipedia) In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. Exploratory data analysis was promoted by John Tukey to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA), which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, handling missing values and making transformations of variables as needed. EDA encompasses IDA. 3.2 The objectives of EDA To Suggest hypotheses about the causes of observed phenomena To Assess assumptions on which statistical inference will be based To Support the selection of appropriate statistical tools and techniques To Provide a basis for further data collection through surveys or experiments 3.3 Data analysis workflow A common workflow for data analysis involves importing data, cleaning data, transforming data, visualizing and modeling data for reports or papers. However, you may notice that this is not a linear process. In other words, there is no magic way of understanding your data with a touch. Rather, it is a process where you need to try different ways of tranforming, visualizing and modeling your data. Therefore, EDA (within the grey square)is not a formal process with a strict set of rules, but an open process. Fortunately, R offers a range of tools that can help us eaily summarize, visualize and model our data. In this workshop, we first explore variation within a variable and then move on to covariation among variables. Figure 3.1: Data analysis workflow "],
["variation.html", "Chapter 4 Variation 4.1 Categorical variable 4.2 Continous variable", " Chapter 4 Variation Variation is the tendency of the values of a variable to change from measurement to measurement. (Wickham &amp; Grolemund) 4.1 Categorical variable The best way to characterizing categorical variables is via the frequency. The frequency is the number of times a particular value for a variable (data item) has been observed to occur. 4.1.1 Absolute v.s relative frequency? The frequency of a value can be expressed in different ways: The absolute frequency describes the number of times a particular value for a variable occurs. Or simply put, counts. The relative frequency describes the number of times a particular value for a variable occurs in relation to the total number of values for that variable.It is calculated by dividing the absolute frequency by the total number of values for the variable. Ratios, rates, proportions and percentages are different ways of expressing relative frequencies. A ratio compares the frequency of one value for a variable with another value for the variable (1st value : 2nd value). For example, in a total of 20 coin tosses where there are 12 heads and 8 tails, the ratio of heads to tails is 12:8. Alternatively, the ratio of tails to heads is 8:12. A rate is a measurement of one value for a variable in relation to another measured quantity. For example, in a total of 20 coin tosses where there are 12 heads and 8 tails, the rate is 12 heads per 20 coin tosses. Alternatively, the rate is 8 tails per 20 coin tosses. A proportion describes the share of one value for a variable in relation to a whole.It is calculated by dividing the number of times a particular value for a variable has been observed, by the total number of values in the population. For example, in a total of 20 coin tosses where there are 12 heads and 8 tails, the proportion of heads is 0.6 (12 divided by 20). Alternatively, the proportion of tails is 0.4 (8 divided by 20). A percentage expresses a value for a variable in relation to a whole population as a fraction of one hundred. The percentage total of an entire dataset should always add up to 100, as 100% represents the total. A percentage is calculated by dividing the number of times a particular value for a variable has been observed, by the total number of observations in the population, then multiplying this number by 100. For example, in a total of 20 coin tosses where there are 12 heads and 8 tails, the percentage of heads is 60% (12 divided by 20, multiplied by 100). Alternatively, the percentage of tails is 40% (8 divided by 20, multiplied by 100). (ABS) 4.1.2 Frequency distributions Frequency distributions are visual displays that organise and present frequency counts so that the information can be interpreted more easily. A frequency distribution of data can be shown in a table or graph. Some common methods of showing frequency distributions include frequency tables, bar charts or histograms. 4.1.2.1 Frequency Tables A frequency table is a simple way to display the number of occurrences of a particular value or characteristic. summary(diamonds) ## carat cut color clarity ## Min. :0.2000 Fair : 1610 D: 6775 SI1 :13065 ## 1st Qu.:0.4000 Good : 4906 E: 9797 VS2 :12258 ## Median :0.7000 Very Good:12082 F: 9542 SI2 : 9194 ## Mean :0.7979 Premium :13791 G:11292 VS1 : 8171 ## 3rd Qu.:1.0400 Ideal :21551 H: 8304 VVS2 : 5066 ## Max. :5.0100 I: 5422 VVS1 : 3655 ## J: 2808 (Other): 2531 ## depth table price x ## Min. :43.00 Min. :43.00 Min. : 326 Min. : 0.000 ## 1st Qu.:61.00 1st Qu.:56.00 1st Qu.: 950 1st Qu.: 4.710 ## Median :61.80 Median :57.00 Median : 2401 Median : 5.700 ## Mean :61.75 Mean :57.46 Mean : 3933 Mean : 5.731 ## 3rd Qu.:62.50 3rd Qu.:59.00 3rd Qu.: 5324 3rd Qu.: 6.540 ## Max. :79.00 Max. :95.00 Max. :18823 Max. :10.740 ## ## y z ## Min. : 0.000 Min. : 0.000 ## 1st Qu.: 4.720 1st Qu.: 2.910 ## Median : 5.710 Median : 3.530 ## Mean : 5.735 Mean : 3.539 ## 3rd Qu.: 6.540 3rd Qu.: 4.040 ## Max. :58.900 Max. :31.800 ## str(diamonds) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 53940 obs. of 10 variables: ## $ carat : num 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... ## $ cut : Ord.factor w/ 5 levels &quot;Fair&quot;&lt;&quot;Good&quot;&lt;..: 5 4 2 4 2 3 3 3 1 3 ... ## $ color : Ord.factor w/ 7 levels &quot;D&quot;&lt;&quot;E&quot;&lt;&quot;F&quot;&lt;&quot;G&quot;&lt;..: 2 2 2 6 7 7 6 5 2 5 ... ## $ clarity: Ord.factor w/ 8 levels &quot;I1&quot;&lt;&quot;SI2&quot;&lt;&quot;SI1&quot;&lt;..: 2 3 5 4 2 6 7 3 4 5 ... ## $ depth : num 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... ## $ table : num 55 61 65 58 58 57 57 55 61 61 ... ## $ price : int 326 326 327 334 335 336 336 337 337 338 ... ## $ x : num 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... ## $ y : num 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... ## $ z : num 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... #library(Hmisc) describe(diamonds) ## diamonds ## ## 10 Variables 53940 Observations ## --------------------------------------------------------------------------- ## carat ## n missing distinct Info Mean Gmd .05 .10 ## 53940 0 273 0.999 0.7979 0.5122 0.30 0.31 ## .25 .50 .75 .90 .95 ## 0.40 0.70 1.04 1.51 1.70 ## ## lowest : 0.20 0.21 0.22 0.23 0.24, highest: 4.00 4.01 4.13 4.50 5.01 ## --------------------------------------------------------------------------- ## cut ## n missing distinct ## 53940 0 5 ## ## Value Fair Good Very Good Premium Ideal ## Frequency 1610 4906 12082 13791 21551 ## Proportion 0.030 0.091 0.224 0.256 0.400 ## --------------------------------------------------------------------------- ## color ## n missing distinct ## 53940 0 7 ## ## Value D E F G H I J ## Frequency 6775 9797 9542 11292 8304 5422 2808 ## Proportion 0.126 0.182 0.177 0.209 0.154 0.101 0.052 ## --------------------------------------------------------------------------- ## clarity ## n missing distinct ## 53940 0 8 ## ## Value I1 SI2 SI1 VS2 VS1 VVS2 VVS1 IF ## Frequency 741 9194 13065 12258 8171 5066 3655 1790 ## Proportion 0.014 0.170 0.242 0.227 0.151 0.094 0.068 0.033 ## --------------------------------------------------------------------------- ## depth ## n missing distinct Info Mean Gmd .05 .10 ## 53940 0 184 0.999 61.75 1.515 59.3 60.0 ## .25 .50 .75 .90 .95 ## 61.0 61.8 62.5 63.3 63.8 ## ## lowest : 43.0 44.0 50.8 51.0 52.2, highest: 72.2 72.9 73.6 78.2 79.0 ## --------------------------------------------------------------------------- ## table ## n missing distinct Info Mean Gmd .05 .10 ## 53940 0 127 0.98 57.46 2.448 54 55 ## .25 .50 .75 .90 .95 ## 56 57 59 60 61 ## ## lowest : 43.0 44.0 49.0 50.0 50.1, highest: 71.0 73.0 76.0 79.0 95.0 ## --------------------------------------------------------------------------- ## price ## n missing distinct Info Mean Gmd .05 .10 ## 53940 0 11602 1 3933 4012 544 646 ## .25 .50 .75 .90 .95 ## 950 2401 5324 9821 13107 ## ## lowest : 326 327 334 335 336, highest: 18803 18804 18806 18818 18823 ## --------------------------------------------------------------------------- ## x ## n missing distinct Info Mean Gmd .05 .10 ## 53940 0 554 1 5.731 1.276 4.29 4.36 ## .25 .50 .75 .90 .95 ## 4.71 5.70 6.54 7.31 7.66 ## ## lowest : 0.00 3.73 3.74 3.76 3.77, highest: 10.01 10.02 10.14 10.23 10.74 ## --------------------------------------------------------------------------- ## y ## n missing distinct Info Mean Gmd .05 .10 ## 53940 0 552 1 5.735 1.269 4.30 4.36 ## .25 .50 .75 .90 .95 ## 4.72 5.71 6.54 7.30 7.65 ## ## Value 0.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 ## Frequency 7 5 1731 12305 7817 5994 6742 9260 4298 3402 ## Proportion 0.000 0.000 0.032 0.228 0.145 0.111 0.125 0.172 0.080 0.063 ## ## Value 8.0 8.5 9.0 9.5 10.0 10.5 32.0 59.0 ## Frequency 1635 652 69 14 6 1 1 1 ## Proportion 0.030 0.012 0.001 0.000 0.000 0.000 0.000 0.000 ## --------------------------------------------------------------------------- ## z ## n missing distinct Info Mean Gmd .05 .10 ## 53940 0 375 1 3.539 0.7901 2.65 2.69 ## .25 .50 .75 .90 .95 ## 2.91 3.53 4.04 4.52 4.73 ## ## Value 0.0 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 ## Frequency 20 1 2 3 8807 13809 9474 13682 5525 2352 ## Proportion 0.000 0.000 0.000 0.000 0.163 0.256 0.176 0.254 0.102 0.044 ## ## Value 5.5 6.0 6.5 7.0 8.0 32.0 ## Frequency 237 20 5 1 1 1 ## Proportion 0.004 0.000 0.000 0.000 0.000 0.000 ## --------------------------------------------------------------------------- contents(diamonds) ## ## Data frame:diamonds 53940 observations and 10 variables Maximum # NAs:0 ## ## ## Levels Class Storage ## carat double ## cut 5 ordered integer ## color 7 ordered integer ## clarity 8 ordered integer ## depth double ## table double ## price integer ## x double ## y double ## z double ## ## +--------+---------------------------------+ ## |Variable|Levels | ## +--------+---------------------------------+ ## | cut |Fair,Good,Very Good,Premium,Ideal| ## +--------+---------------------------------+ ## | color |D,E,F,G,H,I,J | ## +--------+---------------------------------+ ## | clarity|I1,SI2,SI1,VS2,VS1,VVS2,VVS1,IF | ## +--------+---------------------------------+ Categorical variables are usually stored as factors or characters. You can use count() function or bar chart to explore the distribution. 4.1.2.2 Bar chart A bar chart is a type of graph in which each column (plotted either vertically or horizontally) represents a categorical variable or a discrete ungrouped numeric variable. It is used to compare the frequency (count) for a category or characteristic with another category or characteristic.(ABS) How to interprate: In a bar chart, the bar height (if vertical) or length (if horizontal) shows the frequency for each category or characteristic. The distribution of the dataset is not important because the columns each represent an individual category or characteristic rather than intervals for a continuous measurement. Therefore, gaps are included between each bar and each bar can be arranged in any order without affecting the data. count(diamonds, cut) ## # A tibble: 5 x 2 ## cut n ## &lt;ord&gt; &lt;int&gt; ## 1 Fair 1610 ## 2 Good 4906 ## 3 Very Good 12082 ## 4 Premium 13791 ## 5 Ideal 21551 count(diamonds, color) ## # A tibble: 7 x 2 ## color n ## &lt;ord&gt; &lt;int&gt; ## 1 D 6775 ## 2 E 9797 ## 3 F 9542 ## 4 G 11292 ## 5 H 8304 ## 6 I 5422 ## 7 J 2808 ggplot(diamonds)+ geom_bar(aes(x = cut)) ggplot(diamonds)+ geom_bar(aes(x = color)) Try this with clarity! library(languageR) ## ## Attaching package: &#39;languageR&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## english summary(lexdec) ## Subject RT Trial Sex NativeLanguage ## A1 : 79 Min. :5.829 Min. : 23 F:1106 English:948 ## A2 : 79 1st Qu.:6.215 1st Qu.: 64 M: 553 Other :711 ## A3 : 79 Median :6.346 Median :106 ## C : 79 Mean :6.385 Mean :105 ## D : 79 3rd Qu.:6.502 3rd Qu.:146 ## I : 79 Max. :7.587 Max. :185 ## (Other):1185 ## Correct PrevType PrevCorrect Word ## correct :1594 nonword:855 correct :1542 almond : 21 ## incorrect: 65 word :804 incorrect: 117 ant : 21 ## apple : 21 ## apricot : 21 ## asparagus: 21 ## avocado : 21 ## (Other) :1533 ## Frequency FamilySize SynsetCount Length ## Min. :1.792 Min. :0.0000 Min. :0.6931 Min. : 3.000 ## 1st Qu.:3.951 1st Qu.:0.0000 1st Qu.:1.0986 1st Qu.: 5.000 ## Median :4.754 Median :0.0000 Median :1.0986 Median : 6.000 ## Mean :4.751 Mean :0.7028 Mean :1.3154 Mean : 5.911 ## 3rd Qu.:5.652 3rd Qu.:1.0986 3rd Qu.:1.6094 3rd Qu.: 7.000 ## Max. :7.772 Max. :3.3322 Max. :2.3026 Max. :10.000 ## ## Class FreqSingular FreqPlural DerivEntropy ## animal:924 Min. : 4.0 Min. : 0.0 Min. :0.0000 ## plant :735 1st Qu.: 23.0 1st Qu.: 19.0 1st Qu.:0.0000 ## Median : 69.0 Median : 49.0 Median :0.0370 ## Mean : 132.1 Mean :109.7 Mean :0.3856 ## 3rd Qu.: 146.0 3rd Qu.:132.0 3rd Qu.:0.6845 ## Max. :1518.0 Max. :854.0 Max. :2.2641 ## ## Complex rInfl meanRT SubjFreq ## complex: 210 Min. :-1.3437 Min. :6.245 Min. :2.000 ## simplex:1449 1st Qu.:-0.3023 1st Qu.:6.322 1st Qu.:3.160 ## Median : 0.1900 Median :6.364 Median :3.880 ## Mean : 0.2845 Mean :6.379 Mean :3.911 ## 3rd Qu.: 0.6385 3rd Qu.:6.420 3rd Qu.:4.680 ## Max. : 4.4427 Max. :6.621 Max. :6.040 ## ## meanSize meanWeight BNCw BNCc ## Min. :1.323 Min. :0.8244 Min. : 0.02229 Min. : 0.0000 ## 1st Qu.:1.890 1st Qu.:1.4590 1st Qu.: 1.64921 1st Qu.: 0.1625 ## Median :3.099 Median :2.7558 Median : 3.32071 Median : 0.6500 ## Mean :2.891 Mean :2.5516 Mean : 7.37800 Mean : 5.0351 ## 3rd Qu.:3.711 3rd Qu.:3.4178 3rd Qu.: 7.10943 3rd Qu.: 2.9248 ## Max. :4.819 Max. :4.7138 Max. :79.17324 Max. :83.1949 ## ## BNCd BNCcRatio BNCdRatio ## Min. : 0.000 Min. :0.00000 Min. :0.0000 ## 1st Qu.: 1.188 1st Qu.:0.09673 1st Qu.:0.5551 ## Median : 3.800 Median :0.27341 Median :0.9349 ## Mean : 12.995 Mean :0.45834 Mean :1.5428 ## 3rd Qu.: 10.451 3rd Qu.:0.55550 3rd Qu.:2.1315 ## Max. :241.561 Max. :8.29545 Max. :6.3458 ## summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 4.2 Continous variable A continuous variable can take any of an infinite set of ordered values. 4.2.1 Central tendency A measure of central tendency attempts to describe a whole set of data with a single value that represents the middle or centre of its distribution. There are three main measures of central tendency: the mode, the median, and the mean. Each of these measures describes a different indication of the typical or central value in the distribution. 4.2.1.1 Mode The mode is the most commonly occurring value in a distribution. Consider this dataset showing the retirement age of 11 people, in whole years: 54, 54, 54, 55, 56, 57, 57, 58, 58, 60, 60 *Advantage: The mode has an advantage over the median and the mean as it can be found for both numerical and categorical (non-numerical) data. *Limitations: In some distributions, the mode may not reflect the centre of the distribution very well. it is easy to see that the centre of the distribution is 57 years, but the mode is lower, at 54 years. 54, 54, 54, 55, 56, 57, 57, 58, 58, 60, 60 more than one mode for the same distribution of data, bi-modal, or multi-modal. The presence of more than one mode can limit the ability of the mode in describing the centre or typical value of the distribution because a single value to describe the centre cannot be identified. In some cases, particularly where the data are continuous, the distribution may have no mode at all (i.e. if all values are different).In cases such as these, it may be better to consider using the median or mean, or group the data in to appropriate intervals, and find the modal class. 4.2.1.2 Median The median is the middle value in distribution when the values are arranged in ascending or descending order. The median divides the distribution in half (there are 50% of observations on either side of the median value). In a distribution with an odd number of observations, the median value is the middle value. 54, 54, 54, 55, 56, 57, 57, 58, 58, 60, 60 When the distribution has an even number of observations, the median value is the mean of the two middle values. In the following distribution, the two middle values are 56 and 57, therefore the median equals 56.5 years: 52, 54, 54, 54, 55, 56, 57, 57, 58, 58, 60, 60 *Advantage: The median is less affected by outliers and skewed data than the mean, and is usually the preferred measure of central tendency when the distribution is not symmetrical. *Limitation: The median cannot be identified for categorical nominal data, as it cannot be logically ordered. 4.2.1.3 Mean The mean is the sum of the value of each observation in a dataset divided by the number of observations. This is also known as the arithmetic average. Advantage The mean can be used for both continuous and discrete numeric data. Limitations The mean cannot be calculated for categorical data, as the values cannot be summed. As the mean includes every value in the distribution the mean is influenced by outliers and skewed distributions. The population mean is indicated by the Greek symbol µ (pronounced ‘mu’). When the mean is calculated on a distribution from a sample it is indicated by the symbol x̅ (pronounced X-bar). 4.2.2 Measures of Spread Dataset A: 4, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 8 Dataset B: 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11 The measures of central tendency for both datasets above are the same.However, if we look at the spread of the values, we can see that Dataset B is more dispersed than Dataset A. Used together, the measures of central tendency and measures of spread help us to better understand the data Summarising the dataset can help us understand the data, especially when the dataset is large. Measures of spread summarise the data in a way that shows how scattered the values are and how much they differ from the mean value. The spread of the values can be measured for quantitative data, as the variables are numeric and can be arranged into a logical order with a low end value and a high end value.(ABS) Measures of spread include the range, quartiles and the interquartile range, variance and standard deviation.(ABS) dataset1 = c(4, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 8) dataset2 = c(1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11) dataset = data.frame(dataset1, dataset2) dataset = gather(dataset, type, data) ggplot(dataset, aes(data, fill = type)) + geom_bar(position=position_dodge()) #### Range The range is the difference between the smallest value and the largest value in a dataset. range(dataset1) ## [1] 4 8 range(dataset2) ## [1] 1 11 4.2.2.1 Quartiles Quartiles divide an ordered dataset into four equal parts, and refer to the values of the point between the quarters. A dataset may also be divided into quintiles (five equal parts) or deciles (ten equal parts). The lower quartile (Q1) is the point between the lowest 25% of values and the highest 75% of values. It is also called the 25th percentile. The second quartile (Q2) is the middle of the data set. It is also called the 50th percentile, or the median. The upper quartile (Q3) is the point between the lowest 75% and highest 25% of values. It is also called the 75th percentile. 4.2.2.2 The interquartile range The interquartile range (IQR) is the difference between the upper (Q3) and lower (Q1) quartiles, and describes the middle 50% of values when ordered from lowest to highest. The IQR is often seen as a better measure of spread than the range as it is not affected by outliers. quantile(dataset1) ## 0% 25% 50% 75% 100% ## 4 5 6 7 8 quantile(dataset2) ## 0% 25% 50% 75% 100% ## 1.00 3.75 6.00 8.25 11.00 4.2.2.3 Variance vs. standard deviation The variance and the standard deviation are measures of how close each observed data value is to the mean value. In datasets with a small spread all values are very close to the mean, resulting in a small variance and standard deviation. Where a dataset is more dispersed, values are spread further away from the mean, leading to a larger variance and standard deviation. The smaller the variance and standard deviation, the more the mean value is indicative of the whole dataset. Therefore, if all values of a dataset are the same, the standard deviation and variance are zero. The standard deviation is the square root of the variance. The standard deviation of a normal distribution enables us to calculate confidence intervals. In a normal distribution, about 68% of the values are within one standard deviation either side of the mean and about 95% of the scores are within two standard deviations of the mean. var(dataset1) ## [1] 1.272727 sqrt(var(dataset1)) ## [1] 1.128152 sd(dataset1) ## [1] 1.128152 4.2.2.4 Five-number summary The five-number summary is a set of descriptive statistics that provide information about a dataset. It consists of the five most important sample percentiles: the sample minimum (smallest observation) the lower quartile or first quartile the median (the middle value) the upper quartile or third quartile the sample maximum (largest observation) The five-number summary provides a concise summary of the distribution of the observations. Reporting five numbers avoids the need to decide on the most appropriate summary statistic. The five-number summary gives information about the location (from the median), spread (from the quartiles) and range (from the sample minimum and maximum) of the observations. Since it reports order statistics (rather than, say, the mean), the five-number summary is appropriate for ordinal measurements, as well as interval and ratio measurements. (from wikipedia) How to Find a Five-Number Summary: Step 1: Put your numbers in ascending order (from smallest to largest). For this particular data set, the order is: &gt; Example: 1,2,5,6,7,9,12,15,18,19,27 Step 2: Find the minimum and maximum for your data set. Now that your numbers are in order, this should be easy to spot. In the example in step 1, the minimum (the smallest number) is 1 and the maximum (the largest number) is 27. Step 3: Find the median. The median is the middle number. If you aren’t sure how to find the median, see: How to find the mean mode and median. Step 4: Place parentheses around the numbers above and below the median.(This is not technically necessary, but it makes Q1 and Q3 easier to find). (1,2,5,6,7),9,(12,15,18,19,27). Step 5: Find Q1 and Q3. Q1 can be thought of as a median in the lower half of the data, and Q3 can be thought of as a median for the upper half of data. (1,2,5,6,7), 9, ( 12,15,18,19,27). -Step 6: Write down your summary found in the above steps. minimum=1, Q1 =5, median=9, Q3=18, and maximum=27. summary(diamonds$carat) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.2000 0.4000 0.7000 0.7979 1.0400 5.0100 4.2.2.5 Visualization: Boxplot &amp; violin plot The five-number summary can be represented graphically using a boxplot. Boxplots have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles. Outliers may be plotted as individual points. The spacings between the different parts of the box indicate the degree of dispersion (spread) and skewness in the data. box plots typically graph six data points: The lowest value, excluding outliers The first quartile (this is the 25th percentile, or median of all the numbers below the median) The median value (equivalent to the 50th percentile) The third quartile (this is the 75th percentile, or median of all the numbers above the median) The highest value, excluding outliers Outliers source:https://mode.com/blog/how-to-make-box-and-whisker-plot-sql Figure 4.1: Boxplot # r base function boxplot(diamonds$carat) #ggplot version ggplot(diamonds, aes(x=&quot;carat&quot;, y=carat))+ geom_boxplot() A violin plot is a method of plotting numeric data. Typically a violin plot will include all the data that is in a box plot: a marker for the median of the data; a box or marker indicating the interquartile range; and possibly all sample points, if the number of samples is not too high. It is similar to a boxplot, with the addition of a rotated kernel density plot on each side.The difference is particularly useful when the data distribution is multimodal (more than one peak). In this case a violin plot shows the presence of different peaks, their position and relative amplitude. knitr::include_graphics(&quot;img/violinplot.png&quot;) Figure 4.2: Violin plot #source:https://mode.com/blog/violin-plot-examples #violin plot ggplot(diamonds, aes(x=&quot;carat&quot;, y=carat))+ geom_violin() ggplot(diamonds, aes(x=&quot;price&quot;, y=price))+ geom_violin() ### Distributions You can use a histogram and other descriptive stats to characterize its distribution. 4.2.2.6 Histogram A histogram is a type of graph in which each column represents a numeric variable, in particular that which is continuous and/or grouped.It shows the distribution of all observations in a quantitative dataset. It is useful for describing the shape, centre and spread to better understand the distribution of the dataset. hist(rbeta(10000,5,2)) hist(rbeta(10000,2,5)) hist(rbeta(10000,5,5)) How to interprate: The height of the column shows the frequency for a specific range of values. Columns are usually of equal width, however a histogram may show data using unequal ranges (intervals) and therefore have columns of unequal width. The values represented by each column must be mutually exclusive and exhaustive. Therefore, there are no spaces between columns and each observation can only ever belong in one column. It is important that there is no ambiguity in the labelling of the intervals on the x-axis for continuous or grouped data (e.g. 0 to less than 10, 10 to less than 20, 20 to less than 30). # histogram ggplot(diamonds)+ geom_histogram(aes(carat), binwidth = 0.1) # histogram zoom in y axsis ggplot(diamonds)+ geom_histogram(aes(carat), binwidth = 0.1)+ coord_cartesian(ylim = c(0,50)) # histogram zoom in x axsis ggplot(diamonds[ which(diamonds$carat &lt; 3), ])+ geom_histogram(aes(carat), binwidth = 0.1) diamonds%&gt;% filter(carat &lt;3)%&gt;% ggplot()+ geom_histogram(aes(carat), binwidth = 0.1) # density plot ggplot(diamonds,aes(carat))+ geom_density(kernel = &quot;gaussian&quot;) # area ggplot(diamonds,aes(carat))+ geom_area(stat = &quot;bin&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # dotplot ggplot(diamonds,aes(carat))+ geom_dotplot() ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. # freqpoly ggplot(diamonds, aes(carat))+ geom_freqpoly() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 4.2.2.7 Symmetrical distributions: When a distribution is symmetrical, the mode, median and mean are all in the middle of the distribution. The following graph shows a larger retirement age dataset with a distribution which is symmetrical. The mode, median and mean all equal 58 years. Figure 4.3: normal distribution 4.2.2.8 Skewed distributions: When a distribution is skewed, the mode remains the most commonly occurring value, the median remains the middle value in the distribution, but the mean is generally ‘pulled’ in the direction of the tails. In a skewed distribution, the median is often a preferred measure of central tendency, as the mean is not usually in the middle of the distribution. A distribution is said to be positively or right skewed when the tail on the right side of the distribution is longer than the left side. In a positively skewed distribution it is common for the mean to be ‘pulled’ toward the right tail of the distribution. Although there are exceptions to this rule, generally, most of the values, including the median value, tend to be less than the mean value. The following graph shows a larger retirement age data set with a distribution which is right skewed. The data has been grouped into classes, as the variable being measured (retirement age) is continuous. The mode is 54 years, the modal class is 54-56 years, the median is 56 years and the mean is 57.2 years. Figure 4.4: right skewed distribution A distribution is said to be negatively or left skewed when the tail on the left side of the distribution is longer than the right side. In a negatively skewed distribution, it is common for the mean to be ‘pulled’ toward the left tail of the distribution. Although there are exceptions to this rule, generally, most of the values, including the median value, tend to be greater than the mean value. The following graph shows a larger retirement age dataset with a distribution which left skewed. The mode is 65 years, the modal class is 63-65 years, the median is 63 years and the mean is 61.8 years. Figure 4.5: left skewed distribution 4.2.2.9 QQ Plots QQ Plots (Quantile-Quantile plots) are plots of two quantiles against each other. A quantile is a fraction where certain values fall below that quantile. For example, the median is a quantile where 50% of the data fall below that point and 50% lie above it. The purpose of QQ plots is to find out if two sets of data come from the same distribution. A 45 degree angle is plotted on the Q Q plot; if the two data sets come from a common distribution, the points will fall on that reference line.(from Wikipedia) # Solution 1 qplot(sample = carat, data = diamonds) # solution 2 ggplot(diamonds)+ geom_qq(aes(sample = carat))+ geom_qq_line(aes(sample = carat)) 4.2.3 Confidence interval (Taken from Summary and Analysis of Extension Program Evaluation in R by Salvatore S. Mangiafico) A confidence interval is a range in which it is estimated the true population value lies. A confidence interval is used to indicate how accurate a calculated statistic is likely to be. Confidence intervals can be calculated for a variety of statistics, such as the mean, median, or slope of a linear regression. Most of the statistics we use assume we are analyzing a sample which we are using to represent a larger population. 4.2.3.1 Statistics and parameters When we calculate the sample mean, the result is a statistic. It’s an estimate of the population mean, but our calculated sample mean would vary depending on our sample. In theory, there is a mean for the population of interest, and we consider this population mean a parameter. Our goal in calculating the sample mean is estimating the population parameter. 4.2.3.2 Point estimates and confidence intervals Our sample mean is a point estimate for the population parameter. A point estimate is a useful approximation for the parameter, but considering the confidence interval for the estimate gives us more information. As a definition of confidence intervals, if we were to sample the same population many times and calculated a sample mean and a 95% confidence interval each time, then 95% of those intervals would contain the actual population mean. if we want to compare the means of two groups to see if they are statistically different, we will use a t-test, or similar test, calculate a p-value, and draw a conclusion. An alternative approach would be to construct 95% or 99% confidence intervals about the mean for each group. If the confidence intervals of the two means don’t overlap, we are justified in calling them statistically different. 4.2.3.3 Methods The traditional method is appropriate for normally distributed data or with large sample sizes. It produces an interval that is symmetric about the mean. For skewed data, confidence intervals by bootstrapping may be more reliable.For routine use, I recommend using bootstrapped confidence intervals, particularly the BCa or percentile methods. The groupwiseMean function in the rcompanion package can produce confidence intervals both by traditional and bootstrap methods, for grouped and ungrouped data. 4.2.3.3.1 The traditional method The data must be housed in a data frame. By default, the function reports confidence intervals by the traditional method. In the groupwiseMean function, the measurement and grouping variables can be indicated with formula notation, with the measurement variable on the left side of the tilde (~), and grouping variables on the right. The confidence level is indicated by, e.g., the conf = 0.95 argument. The digits option indicates the number of significant digits to which the output is rounded. Note that in the output, the means and other statistics are rounded to 3 significant figures. #install.packages(&quot;rcompanion&quot;) library(rcompanion) dataset3 = data.frame(dataset1) groupwiseMean(dataset1 ~ 1, data = dataset3, conf = 0.95, digits = 3) ## .id n Mean Conf.level Trad.lower Trad.upper ## 1 &lt;NA&gt; 12 6 0.95 5.28 6.72 groupwiseMean(data ~ type, data = dataset, conf = 0.95, digits = 3) ## type n Mean Conf.level Trad.lower Trad.upper ## 1 dataset1 12 6 0.95 5.28 6.72 ## 2 dataset2 12 6 0.95 3.99 8.01 4.2.3.3.2 The Bootstrapped method In the groupwiseMean function, the type of confidence interval is requested by setting certain options to TRUE. These options are traditional, normal, basic, percentile and bca. The boot option reports an optional statistic, the mean by bootstrap. The R option indicates the number of iterations to calculate each bootstrap statistic. groupwiseMean(data ~ type, data = dataset, conf = 0.95, digits = 3, R = 10000, boot = TRUE, traditional = FALSE, normal = FALSE, basic = FALSE, percentile = FALSE, bca = TRUE) ## type n Mean Boot.mean Conf.level Bca.lower Bca.upper ## 1 dataset1 12 6 6 0.95 5.33 6.58 ## 2 dataset2 12 6 6 0.95 4.25 7.58 4.2.4 Unusual values For unusual values in the dataset, you can do one of the two things: drop the entire row with the strange values replace the unusual values with missing values unusual = diamonds %&gt;% filter(y &lt; 3 | y &gt; 20)%&gt;% arrange(y) unusual ## # A tibble: 9 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Very Good H VS2 63.3 53 5139 0 0 0 ## 2 1.14 Fair G VS1 57.5 67 6381 0 0 0 ## 3 1.56 Ideal G VS2 62.2 54 12800 0 0 0 ## 4 1.2 Premium D VVS1 62.1 59 15686 0 0 0 ## 5 2.25 Premium H SI2 62.8 59 18034 0 0 0 ## 6 0.71 Good F SI2 64.1 60 2130 0 0 0 ## 7 0.71 Good F SI2 64.1 60 2130 0 0 0 ## 8 0.51 Ideal E VS1 61.8 55 2075 5.15 31.8 5.12 ## 9 2 Premium H SI2 58.9 57 12210 8.09 58.9 8.06 # drop the entire row with the strange values diamonds_new = diamonds%&gt;% filter(between(y,3,20)) head(arrange(diamonds_new,y)) ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.2 Premium D VS2 62.3 60 367 3.73 3.68 2.31 ## 2 0.2 Premium F VS2 62.6 59 367 3.73 3.71 2.33 ## 3 0.2 Very Good E VS2 63.4 59 367 3.74 3.71 2.36 ## 4 0.2 Premium D VS2 61.7 60 367 3.77 3.72 2.31 ## 5 0.2 Ideal E VS2 62.2 57 367 3.76 3.73 2.33 ## 6 0.2 Premium E SI2 60.2 62 345 3.79 3.75 2.27 # replace the unusual values with missing values diamonds_replace = diamonds %&gt;% mutate(y2 = ifelse(y&lt;3 | y&gt; 20, NA, y)) head(arrange(diamonds_replace, y )) ## # A tibble: 6 x 11 ## carat cut color clarity depth table price x y z y2 ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Very Good H VS2 63.3 53 5139 0 0 0 NA ## 2 1.14 Fair G VS1 57.5 67 6381 0 0 0 NA ## 3 1.56 Ideal G VS2 62.2 54 12800 0 0 0 NA ## 4 1.2 Premium D VVS1 62.1 59 15686 0 0 0 NA ## 5 2.25 Premium H SI2 62.8 59 18034 0 0 0 NA ## 6 0.71 Good F SI2 64.1 60 2130 0 0 0 NA "],
["covariation.html", "Chapter 5 Covariation 5.1 Two categorical variables 5.2 Categorical + continuous variable 5.3 Two continuous variables", " Chapter 5 Covariation While the variation describes the behaviors within a variable, covariation describes the behavior between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way. 5.1 Two categorical variables 5.1.1 Contingency table To characterize covaritation of two categorical variable, we can use contingency table to display the frequency. table(diamonds$cut, diamonds$color) ## ## D E F G H I J ## Fair 163 224 312 314 303 175 119 ## Good 662 933 909 871 702 522 307 ## Very Good 1513 2400 2164 2299 1824 1204 678 ## Premium 1603 2337 2331 2924 2360 1428 808 ## Ideal 2834 3903 3826 4884 3115 2093 896 table(diamonds$cut, diamonds$color, diamonds$clarity) ## , , = I1 ## ## ## D E F G H I J ## Fair 4 9 35 53 52 34 23 ## Good 8 23 19 19 14 9 4 ## Very Good 5 22 13 16 12 8 8 ## Premium 12 30 34 46 46 24 13 ## Ideal 13 18 42 16 38 17 2 ## ## , , = SI2 ## ## ## D E F G H I J ## Fair 56 78 89 80 91 45 27 ## Good 223 202 201 163 158 81 53 ## Very Good 314 445 343 327 343 200 128 ## Premium 421 519 523 492 521 312 161 ## Ideal 356 469 453 486 450 274 110 ## ## , , = SI1 ## ## ## D E F G H I J ## Fair 58 65 83 69 75 30 28 ## Good 237 355 273 207 235 165 88 ## Very Good 494 626 559 474 547 358 182 ## Premium 556 614 608 566 655 367 209 ## Ideal 738 766 608 660 763 504 243 ## ## , , = VS2 ## ## ## D E F G H I J ## Fair 25 42 53 45 41 32 23 ## Good 104 160 184 192 138 110 90 ## Very Good 309 503 466 479 376 274 184 ## Premium 339 629 619 721 532 315 202 ## Ideal 920 1136 879 910 556 438 232 ## ## , , = VS1 ## ## ## D E F G H I J ## Fair 5 14 33 45 32 25 16 ## Good 43 89 132 152 77 103 52 ## Very Good 175 293 293 432 257 205 120 ## Premium 131 292 290 566 336 221 153 ## Ideal 351 593 616 953 467 408 201 ## ## , , = VVS2 ## ## ## D E F G H I J ## Fair 9 13 10 17 11 8 1 ## Good 25 52 50 75 45 26 13 ## Very Good 141 298 249 302 145 71 29 ## Premium 94 121 146 275 118 82 34 ## Ideal 284 507 520 774 289 178 54 ## ## , , = VVS1 ## ## ## D E F G H I J ## Fair 3 3 5 3 1 1 1 ## Good 13 43 35 41 31 22 1 ## Very Good 52 170 174 190 115 69 19 ## Premium 40 105 80 171 112 84 24 ## Ideal 144 335 440 594 326 179 29 ## ## , , = IF ## ## ## D E F G H I J ## Fair 3 0 4 2 0 0 0 ## Good 9 9 15 22 4 6 6 ## Very Good 23 43 67 79 29 19 8 ## Premium 10 27 31 87 40 23 12 ## Ideal 28 79 268 491 226 95 25 xtabs(~ cut + color, diamonds) ## color ## cut D E F G H I J ## Fair 163 224 312 314 303 175 119 ## Good 662 933 909 871 702 522 307 ## Very Good 1513 2400 2164 2299 1824 1204 678 ## Premium 1603 2337 2331 2924 2360 1428 808 ## Ideal 2834 3903 3826 4884 3115 2093 896 xtabs(~ cut + color + clarity, diamonds) ## , , clarity = I1 ## ## color ## cut D E F G H I J ## Fair 4 9 35 53 52 34 23 ## Good 8 23 19 19 14 9 4 ## Very Good 5 22 13 16 12 8 8 ## Premium 12 30 34 46 46 24 13 ## Ideal 13 18 42 16 38 17 2 ## ## , , clarity = SI2 ## ## color ## cut D E F G H I J ## Fair 56 78 89 80 91 45 27 ## Good 223 202 201 163 158 81 53 ## Very Good 314 445 343 327 343 200 128 ## Premium 421 519 523 492 521 312 161 ## Ideal 356 469 453 486 450 274 110 ## ## , , clarity = SI1 ## ## color ## cut D E F G H I J ## Fair 58 65 83 69 75 30 28 ## Good 237 355 273 207 235 165 88 ## Very Good 494 626 559 474 547 358 182 ## Premium 556 614 608 566 655 367 209 ## Ideal 738 766 608 660 763 504 243 ## ## , , clarity = VS2 ## ## color ## cut D E F G H I J ## Fair 25 42 53 45 41 32 23 ## Good 104 160 184 192 138 110 90 ## Very Good 309 503 466 479 376 274 184 ## Premium 339 629 619 721 532 315 202 ## Ideal 920 1136 879 910 556 438 232 ## ## , , clarity = VS1 ## ## color ## cut D E F G H I J ## Fair 5 14 33 45 32 25 16 ## Good 43 89 132 152 77 103 52 ## Very Good 175 293 293 432 257 205 120 ## Premium 131 292 290 566 336 221 153 ## Ideal 351 593 616 953 467 408 201 ## ## , , clarity = VVS2 ## ## color ## cut D E F G H I J ## Fair 9 13 10 17 11 8 1 ## Good 25 52 50 75 45 26 13 ## Very Good 141 298 249 302 145 71 29 ## Premium 94 121 146 275 118 82 34 ## Ideal 284 507 520 774 289 178 54 ## ## , , clarity = VVS1 ## ## color ## cut D E F G H I J ## Fair 3 3 5 3 1 1 1 ## Good 13 43 35 41 31 22 1 ## Very Good 52 170 174 190 115 69 19 ## Premium 40 105 80 171 112 84 24 ## Ideal 144 335 440 594 326 179 29 ## ## , , clarity = IF ## ## color ## cut D E F G H I J ## Fair 3 0 4 2 0 0 0 ## Good 9 9 15 22 4 6 6 ## Very Good 23 43 67 79 29 19 8 ## Premium 10 27 31 87 40 23 12 ## Ideal 28 79 268 491 226 95 25 ftable(xtabs(~ cut + color + clarity, diamonds)) ## clarity I1 SI2 SI1 VS2 VS1 VVS2 VVS1 IF ## cut color ## Fair D 4 56 58 25 5 9 3 3 ## E 9 78 65 42 14 13 3 0 ## F 35 89 83 53 33 10 5 4 ## G 53 80 69 45 45 17 3 2 ## H 52 91 75 41 32 11 1 0 ## I 34 45 30 32 25 8 1 0 ## J 23 27 28 23 16 1 1 0 ## Good D 8 223 237 104 43 25 13 9 ## E 23 202 355 160 89 52 43 9 ## F 19 201 273 184 132 50 35 15 ## G 19 163 207 192 152 75 41 22 ## H 14 158 235 138 77 45 31 4 ## I 9 81 165 110 103 26 22 6 ## J 4 53 88 90 52 13 1 6 ## Very Good D 5 314 494 309 175 141 52 23 ## E 22 445 626 503 293 298 170 43 ## F 13 343 559 466 293 249 174 67 ## G 16 327 474 479 432 302 190 79 ## H 12 343 547 376 257 145 115 29 ## I 8 200 358 274 205 71 69 19 ## J 8 128 182 184 120 29 19 8 ## Premium D 12 421 556 339 131 94 40 10 ## E 30 519 614 629 292 121 105 27 ## F 34 523 608 619 290 146 80 31 ## G 46 492 566 721 566 275 171 87 ## H 46 521 655 532 336 118 112 40 ## I 24 312 367 315 221 82 84 23 ## J 13 161 209 202 153 34 24 12 ## Ideal D 13 356 738 920 351 284 144 28 ## E 18 469 766 1136 593 507 335 79 ## F 42 453 608 879 616 520 440 268 ## G 16 486 660 910 953 774 594 491 ## H 38 450 763 556 467 289 326 226 ## I 17 274 504 438 408 178 179 95 ## J 2 110 243 232 201 54 29 25 diamonds %&gt;% group_by(cut, color)%&gt;% count( )%&gt;% group_by(cut)%&gt;% mutate(sum = sum(n))%&gt;% mutate(proportion = n/sum, percentage = (n/sum)*100) ## # A tibble: 35 x 6 ## # Groups: cut [5] ## cut color n sum proportion percentage ## &lt;ord&gt; &lt;ord&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Fair D 163 1610 0.101 10.1 ## 2 Fair E 224 1610 0.139 13.9 ## 3 Fair F 312 1610 0.194 19.4 ## 4 Fair G 314 1610 0.195 19.5 ## 5 Fair H 303 1610 0.188 18.8 ## 6 Fair I 175 1610 0.109 10.9 ## 7 Fair J 119 1610 0.0739 7.39 ## 8 Good D 662 4906 0.135 13.5 ## 9 Good E 933 4906 0.190 19.0 ## 10 Good F 909 4906 0.185 18.5 ## # ... with 25 more rows 5.1.2 Tile plot You can visualize the frequency table with a tile plot. diamonds%&gt;% count(color, cut)%&gt;% ggplot(aes(color, cut))+ geom_tile(aes(fill=n)) ggplot(diamonds)+ geom_count(aes(cut, color)) #install.packages(&quot;seriation&quot;) 5.2 Categorical + continuous variable The covariation of a categorical and a continuous variable can be visualized and explored by treating the categorical variable as the group factor. Then we can apply all the methods we learned when dealing with a continuous variable. 5.2.1 Summary table R offers a number of ways we can summarize the mean, sd of a continous variable as a function of one or more grouping variables. # solution 1 with(diamonds, tapply(price, cut, mean)) ## Fair Good Very Good Premium Ideal ## 4358.758 3928.864 3981.760 4584.258 3457.542 with(diamonds, tapply(price, list(cut, color, clarity), mean)) ## , , I1 ## ## D E F G H I J ## Fair 7383.000 2095.222 2543.514 3187.472 4212.962 3501.000 5795.043 ## Good 3490.750 4398.130 2569.526 3195.789 3849.714 4175.444 3794.500 ## Very Good 2622.800 3443.545 4252.923 3194.812 5258.833 6045.125 4478.375 ## Premium 3818.750 3199.267 3554.559 4051.522 3904.348 5044.625 4577.231 ## Ideal 3526.923 3559.389 3903.452 4044.438 5415.184 4103.294 9454.000 ## ## , , SI2 ## ## D E F G H I J ## Fair 4355.143 4172.385 4520.112 5665.150 6022.407 6658.022 5131.815 ## Good 3595.296 3785.490 4426.786 4776.411 5529.778 6933.012 5306.113 ## Very Good 4425.459 4279.447 4249.758 4699.269 6112.414 6621.600 5992.898 ## Premium 4351.086 4489.931 4747.090 5617.205 6718.946 7148.484 7550.286 ## Ideal 3142.048 3891.303 4335.508 4612.086 5589.473 7191.912 6555.173 ## ## , , SI1 ## ## D E F G H I J ## Fair 4273.345 3901.154 3784.687 3579.362 5195.800 4574.967 4553.929 ## Good 3021.173 3162.132 3261.454 4129.329 4179.285 4742.945 4627.625 ## Very Good 3234.931 3228.176 3574.292 3481.871 4933.945 5195.302 5026.544 ## Premium 3236.378 3362.625 4040.467 4303.348 5707.722 6092.093 5726.579 ## Ideal 2490.459 2883.808 3710.322 3441.108 4769.988 5178.565 5115.675 ## ## , , VS2 ## ## D E F G H I J ## Fair 4512.880 3041.714 3400.472 5384.444 5110.927 3856.125 4067.826 ## Good 3588.462 3772.019 3790.543 4140.714 4433.043 5956.564 4803.167 ## Very Good 3145.194 3329.497 3995.944 4426.816 4620.221 5754.642 5325.549 ## Premium 2919.357 3070.394 4221.467 4556.255 5553.876 7156.346 6175.559 ## Ideal 2111.927 2163.324 3317.205 4310.035 4039.126 4663.384 4867.134 ## ## , , VS1 ## ## D E F G H I J ## Fair 2921.200 3307.929 4103.061 3497.622 4604.750 4500.480 5906.188 ## Good 3556.581 3712.775 2787.508 4302.428 3819.117 4597.165 3662.827 ## Very Good 2955.480 3089.358 3880.802 3770.150 3750.198 5276.971 4339.592 ## Premium 4178.046 3721.695 4758.038 4435.823 3949.336 5339.367 5817.261 ## Ideal 2576.040 2175.798 3504.002 4116.918 3613.325 3944.422 4734.428 ## ## , , VVS2 ## ## D E F G H I J ## Fair 3607.000 3119.308 4018.200 3099.059 3481.727 2994.625 2998.000 ## Good 2345.640 3390.154 3192.360 3310.467 2428.000 2758.000 4371.154 ## Very Good 2615.298 2041.685 3461.912 3711.785 2768.145 3059.887 5960.448 ## Premium 3888.436 2940.942 4099.466 4323.571 2651.263 3190.768 6423.353 ## Ideal 3619.014 2556.335 3323.629 3795.651 2591.156 2858.680 4121.926 ## ## , , VVS1 ## ## D E F G H I J ## Fair 4473.000 4115.333 4679.800 2216.333 4115.000 4194.000 1691.000 ## Good 2586.231 1905.953 2189.514 2705.195 1719.710 2650.955 4633.000 ## Very Good 2987.731 1997.447 2826.540 2719.332 2042.191 2056.420 3175.526 ## Premium 3771.000 2699.857 3969.325 2933.655 1453.759 1831.083 7244.375 ## Ideal 2705.778 2205.519 2611.234 2909.199 1915.985 2034.397 2000.172 ## ## , , IF ## ## D E F G H I J ## Fair 1619.667 NA 2344.000 1488.000 NA NA NA ## Good 10030.333 1519.222 3132.867 4060.136 5948.750 1749.333 2738.000 ## Very Good 10298.261 4332.744 4677.075 3525.241 2647.690 4093.895 1074.125 ## Premium 9056.500 4525.444 3617.581 3311.115 3384.750 2358.565 7026.000 ## Ideal 6567.179 3258.937 2153.709 2206.031 1982.765 1502.621 2489.000 # solution 2 # install.packages(&quot;doBy&quot;) library(doBy) data = as.data.frame(diamonds) head(summaryBy(price ~ cut + clarity + color , data = data, FUN = mean)) ## cut clarity color price.mean ## 1 Fair I1 D 7383.000 ## 2 Fair I1 E 2095.222 ## 3 Fair I1 F 2543.514 ## 4 Fair I1 G 3187.472 ## 5 Fair I1 H 4212.962 ## 6 Fair I1 I 3501.000 head(summaryBy(price + carat ~ cut + clarity + color , data = data, FUN = mean)) ## cut clarity color price.mean carat.mean ## 1 Fair I1 D 7383.000 1.8775000 ## 2 Fair I1 E 2095.222 0.9688889 ## 3 Fair I1 F 2543.514 1.0234286 ## 4 Fair I1 G 3187.472 1.2264151 ## 5 Fair I1 H 4212.962 1.4986538 ## 6 Fair I1 I 3501.000 1.3229412 head(summaryBy(price + carat ~ cut + color , data = data, FUN = c(mean, sd))) ## cut color price.mean carat.mean price.sd carat.sd ## 1 Fair D 4291.061 0.9201227 3286.114 0.4054185 ## 2 Fair E 3682.312 0.8566071 2976.652 0.3645848 ## 3 Fair F 3827.003 0.9047115 3223.303 0.4188899 ## 4 Fair G 4239.255 1.0238217 3609.644 0.4927241 ## 5 Fair H 5135.683 1.2191749 3886.482 0.5482389 ## 6 Fair I 4685.446 1.1980571 3730.271 0.5219776 # solution 3 diamonds%&gt;% group_by(cut, clarity, color)%&gt;% summarise(mean = mean(price), sd = sd(price)) ## # A tibble: 276 x 5 ## # Groups: cut, clarity [40] ## cut clarity color mean sd ## &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Fair I1 D 7383 5899. ## 2 Fair I1 E 2095. 824. ## 3 Fair I1 F 2544. 2227. ## 4 Fair I1 G 3187. 2598. ## 5 Fair I1 H 4213. 3149. ## 6 Fair I1 I 3501 2157. ## 7 Fair I1 J 5795. 4594. ## 8 Fair SI2 D 4355. 3260. ## 9 Fair SI2 E 4172. 3055. ## 10 Fair SI2 F 4520. 3627. ## # ... with 266 more rows 5.2.2 Central tendency (mean): Bar plots #bar plot diamonds%&gt;% group_by(cut)%&gt;% summarise(mean = mean(price))%&gt;% ggplot(aes(cut, mean))+ geom_bar(stat=&quot;identity&quot;) diamonds%&gt;% group_by(cut)%&gt;% summarise(mean = mean(price))%&gt;% ggplot(aes(cut, mean, fill = cut))+ geom_bar(stat=&quot;identity&quot;) # xlim(4, 10) + ylim(4, 10) diamonds%&gt;% group_by(cut)%&gt;% summarise(mean = mean(price), sd = sd(price))%&gt;% ggplot(aes(cut, mean))+ geom_bar(stat=&quot;identity&quot;)+ geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = .2, size = 0.7, position = position_dodge(.9)) ### Spread: boxplot # boxplot ggplot(diamonds, aes(cut, price))+ geom_boxplot() 5.2.3 Distribution: density plot We can color-code the density plot to represent the group factor. library(tidyverse) ggplot(diamonds, aes(price))+ geom_freqpoly(binwidth = 500) ggplot(diamonds, aes(price))+ geom_freqpoly(aes(color = cut), binwidth = 500) # standardized count where the area under each frequency polygon is one ggplot(diamonds, aes(x = price, y = ..density..))+ geom_freqpoly(aes(color = cut), binwidth = 500) model = model name displ = engine displacement, in litres year = year of manufacture cyl = number of cylinders trans = type of transmission drv =&gt; f = front-wheel drive, r = rear wheel drive, 4 = 4wd cty = city miles per gallon hwy = highway miles per gallon fl = fuel type class = “type” of car summary(mpg) ## manufacturer model displ year ## Length:234 Length:234 Min. :1.600 Min. :1999 ## Class :character Class :character 1st Qu.:2.400 1st Qu.:1999 ## Mode :character Mode :character Median :3.300 Median :2004 ## Mean :3.472 Mean :2004 ## 3rd Qu.:4.600 3rd Qu.:2008 ## Max. :7.000 Max. :2008 ## cyl trans drv cty ## Min. :4.000 Length:234 Length:234 Min. : 9.00 ## 1st Qu.:4.000 Class :character Class :character 1st Qu.:14.00 ## Median :6.000 Mode :character Mode :character Median :17.00 ## Mean :5.889 Mean :16.86 ## 3rd Qu.:8.000 3rd Qu.:19.00 ## Max. :8.000 Max. :35.00 ## hwy fl class ## Min. :12.00 Length:234 Length:234 ## 1st Qu.:18.00 Class :character Class :character ## Median :24.00 Mode :character Mode :character ## Mean :23.44 ## 3rd Qu.:27.00 ## Max. :44.00 ggplot(mpg)+ geom_boxplot(aes(x = reorder(class, hwy, FUN = median), y = hwy)) ggplot(mpg)+ geom_boxplot(aes(x = reorder(class, hwy, FUN = median), y = hwy))+ coord_flip() 5.3 Two continuous variables 5.3.1 Scatter plots The most common way we visualize two continuous variables is by using a scatter plot. ggplot(diamonds)+ geom_point(aes(carat, price)) # add transparency ggplot(diamonds)+ geom_point(aes(carat, price), alpha = 1/100) # bin two variables ggplot(diamonds)+ geom_bin2d(aes(carat, price)) #install.packages(&quot;hexbin&quot;) ggplot(diamonds)+ geom_hex(aes(carat, price)) #bin one variable ggplot(diamonds,aes(carat, price))+ geom_boxplot(aes(group = cut_width(carat, 0.1))) ggplot(diamonds,aes(carat, price))+ geom_boxplot(aes(group = cut_width(carat, 0.5))) ### Bin one or both continuous variables Sometime we can bin one or both continuous variables to convert them into categorical variable(s). In those cases, we apply what we learn in dealing with categorical variables, such as tile plots or boxplots. #bin one variable ggplot(diamonds,aes(carat, price))+ geom_boxplot(aes(group = cut_width(carat, 0.1))) ggplot(diamonds,aes(carat, price))+ geom_boxplot(aes(group = cut_width(carat, 0.5))) # bin two variables ggplot(diamonds)+ geom_bin2d(aes(carat, price)) #install.packages(&quot;hexbin&quot;) ggplot(diamonds)+ geom_hex(aes(carat, price)) "],
["from-data-visualization-to-statistical-modelling.html", "Chapter 6 From data visualization to statistical modelling 6.1 Two continuous variables", " Chapter 6 From data visualization to statistical modelling Patterns in the data provide clues about relationship or covariation.Now that we know how to visualize the various relationships, we can proceed to learn more about how to formally test the relationship. Statistical models are tools for extracting patterns out of data. Statistics represent a common method of presenting information helping us to understand what the data are telling us. Descriptive (or summary) statistics summarise the raw data and allow data users to interpret a dataset more easily.Descriptive statistics can describe the shape, centre and spread of a dataset. Inferential statistics are used to infer conclusions about a population from a sample of that population. It includes estimation (An estimate is a value that is inferred for a population based on data collected from a sample of units from that population), and hypothesis testing. Figure 6.1: stats 6.1 Two continuous variables 6.1.1 Simple linear regression The techique we used here is called Simple linear regression, where there is one dependent variable (continuous) and one independent variable (continuous). When there are more than one independent variable (continuous), you need to look for something called Multiple linear regression. head(faithful) ## eruptions waiting ## 1 3.600 79 ## 2 1.800 54 ## 3 3.333 74 ## 4 2.283 62 ## 5 4.533 85 ## 6 2.883 55 ggplot(faithful)+ geom_point(aes(eruptions, waiting)) 6.1.2 correlation vs. linear regression Correlation and linear regression each explore the relationship between two quantitative variables. (Salvatore S. Mangiafico) Correlation determines if one variable varies systematically as another variable changes. It does not specify that one variable is the dependent variable and the other is the independent variable. Often, it is useful to look at which variables are correlated to others in a data set, and it is especially useful to see which variables correlate to a particular variable of interest. In contrast, linear regression specifies one variable as the independent variable and another as the dependent variable. The resultant model relates the variables with a linear relationship. The tests associated with linear regression are parametric and assume normality, homoscedasticity, and independence of residuals, as well as a linear relationship between the two variables. ## Call:corr.test(x = faithful, use = &quot;pairwise&quot;, method = &quot;pearson&quot;, ## adjust = &quot;none&quot;) ## Correlation matrix ## eruptions waiting ## eruptions 1.0 0.9 ## waiting 0.9 1.0 ## Sample Size ## [1] 272 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## eruptions waiting ## eruptions 0 0 ## waiting 0 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option ## Warning in cor.test.default(as.numeric(x), as.numeric(y), method = method): ## Cannot compute exact p-value with ties ### Pearson, Spearman, and Kendall regression 6.1.2.1 Effect size The statistics r, rho, and tau are used as effect sizes for Pearson, Spearman, and Kendall regression, respectively. These statistics vary from –1 to 1, with 0 indicating no correlation, 1 indicating a perfect positive correlation, and –1 indicating a perfect negative correlation. Like other effect size statistics, these statistics are not affected by sample size. Interpretation of effect sizes necessarily varies by discipline and the expectations of the experiment. They should not be considered universal. An interpretation of r is given by Cohen (1988). It is probably reasonable to use similar interpretations for rho and tau. small: 0.10 – &lt; 0.30 medium: 0.30 – &lt; 0.50 large: ≥ 0.50 6.1.2.2 Pearson correlation The test used for Pearson correlation is a parametric analysis that requires that the relationship between the variables is linear, and that the data be bivariate normal. Variables should be interval/ratio. The test is sensitive to outliers. The correlation coefficient, r, can range from +1 to –1, with +1 being a perfect positive correlation and –1 being a perfect negative correlation. An r of 0 represents no correlation whatsoever. The hypothesis test determines if the r value is significantly different from 0. cor.test( ~ eruptions + waiting, data=faithful, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: eruptions and waiting ## t = 34.089, df = 270, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.8756964 0.9210652 ## sample estimates: ## cor ## 0.9008112 # the results report the p-value for the hypothesis test as well as the r value, written as cor, 0.849. 6.1.2.3 Kendall correlation Kendall correlation is considered a nonparametric analysis. - It is a rank-based test that does not require assumptions about the distribution of the data. - Variables can be interval/ratio or ordinal. The correlation coefficient from the test is tau, which can range from +1 to –1, with +1 being a perfect positive correlation and –1 being a perfect negative correlation. A tau of 0 represents no correlation whatsoever. The hypothesis test determines if the tau value is significantly different from 0. As a technical note, the cor.test function in R calculates tau-b, which handles ties in ranks well. The test is relatively robust to outliers in the data. The test is sometimes cited for being reliable when there are small number of samples or when there are many ties in ranks. cor.test( ~ eruptions + waiting, data=faithful, method = &quot;kendall&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: eruptions and waiting ## z = 13.902, p-value &lt; 2.2e-16 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.5747674 6.1.2.4 Spearman correlation Spearman correlation is considered a nonparametric analysis. It is a rank-based test that does not require assumptions about the distribution of the data. Variables can be interval/ratio or ordinal. The correlation coefficient from the test, rho, can range from +1 to –1, with +1 being a perfect positive correlation and –1 being a perfect negative correlation. A rho of 0 represents no correlation whatsoever. The hypothesis test determines if the rho value is significantly different from 0. Spearman correlation is probably most often used with ordinal data. It tests for a monotonic relationship between the variables. It is relatively robust to outliers in the data. cor.test( ~ eruptions + waiting, data=faithful, method = &quot;spearman&quot;) ## Warning in cor.test.default(x = c(3.6, 1.8, 3.333, 2.283, 4.533, 2.883, : ## Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: eruptions and waiting ## S = 744660, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.7779721 6.1.2.5 Linear regression Linear regression is a very common approach to model the relationship between two interval/ratio variables. The outcome of linear regression includes estimating the intercept and the slope of the linear model. Multiple, nominal, and ordinal independent variables If there are multiple independent variables of interval/ratio type in the model, then linear regression expands to multiple regression. If the independent variable were of nominal type, then the linear regression would become a one-way analysis of variance. Handling independent variables of ordinal type can be complicated. Often they are treated as either nominal type or interval/ratio type, although there are drawbacks to each approach. Assumptions Linear regression assumes - a linear relationship between the two variables, - normality of the residuals, - independence of the residuals, - homoscedasticity of residuals. Linear regression can be performed with the lm function, which was the same function we used for analysis of variance. model = lm(eruptions ~ waiting, data = faithful) summary(model) ## ## Call: ## lm(formula = eruptions ~ waiting, data = faithful) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.29917 -0.37689 0.03508 0.34909 1.19329 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.874016 0.160143 -11.70 &lt;2e-16 *** ## waiting 0.075628 0.002219 34.09 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4965 on 270 degrees of freedom ## Multiple R-squared: 0.8115, Adjusted R-squared: 0.8108 ## F-statistic: 1162 on 1 and 270 DF, p-value: &lt; 2.2e-16 The summary function for lm model objects includes estimates for model parameters (intercept and slope), as well as an r-squared value for the model and p-value for the model. How to read the model? The model produces a coefficient for the intercept (-1.87) and a coefficient for the slope (0.07); Each coefficient comes with three other numbers: its standard error, a t-value, and a p-value. The p-value tells us whether the coefficient is significantly different from zero. If the coefficient for a predictor is zero, there is no relation at all between the predictor and the dependent variable, in which case it is worthless as a predictor. In order to ascertain whether a coefficient is significantly different from zero, and hence potentially useful, a two-tailed t-test is carried out, using the t-value and the associated degrees of freedom. The t-value itself is the value of the coefficient divided by its standard error. This standard error is a measure of how sure we are about the estimate of the coefficient. The smaller the standard error, the smaller the confidence interval around the estimate, the less likely it is that zero will be included in the acceptance region, and hence the smaller the probability that it might just as well be zero. The residual standard error is a measure of how unsuccessful the model is; it gauges the variability in the dependent variable that we can’t handle through the predictor variables. The better a model is, the smaller its residual standard error will be. The multiple R-squared equals 0.8115. This R-squared is the squared correlation coefficient, r2, which quantifies, on a scale from 0 to 1, the proportion of the variance that the model explains. plot(eruptions ~ waiting, data=faithful, pch=16, xlab = &quot;waiting&quot;, ylab = &quot;eruptions&quot;) abline(model, col = &quot;blue&quot;, lwd = 2) x = residuals(model) #library(rcompanion) plotNormalHistogram(x) 6.1.3 Categorical (independent variable ) + continuous (dependent variable) 6.1.3.1 T-tests T-tests are commonly used to compare the means of two samples or between one sample and a fixed value. In other words, the independent variable should be categorical and have two levels. 6.1.3.1.1 Requirements: Observations between groups are independent.That is, not paired or repeated measures data Data for each population are normally distributed.Moderate skewness is permissible if the data distribution is unimodal without outliers. # distribution plot(density(df$chinese)) qqnorm(df$chinese) # formal tests # null hypothesis: the distribution is normal shapiro.test(df$chinese) ## ## Shapiro-Wilk normality test ## ## data: df$chinese ## W = 0.98804, p-value = 0.5113 ks.test(df$chinese, &quot;pnorm&quot;, mean(df$chinese), sd(df$chinese)) ## ## One-sample Kolmogorov-Smirnov test ## ## data: df$chinese ## D = 0.054443, p-value = 0.9283 ## alternative hypothesis: two-sided For Student’s t-test, the two samples need to have the same variance. However, Welch’s t-test, which is used by default in R, does not assume equal variances. #independent sample t-test---compare means class1 = df[df$class == 1, ]$chinese class2 = df[df$class == 2, ]$chinese var.test(class1,class2)#compare variance ## ## F test to compare two variances ## ## data: class1 and class2 ## F = 2.035, num df = 24, denom df = 24, p-value = 0.08822 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.8967588 4.6179663 ## sample estimates: ## ratio of variances ## 2.034994 6.1.3.1.2 One-sample t-test #one sample t-test: one group and a fixed value t.test(df$chinese, mu = 78) ## ## One Sample t-test ## ## data: df$chinese ## t = 4.7742, df = 99, p-value = 6.244e-06 ## alternative hypothesis: true mean is not equal to 78 ## 95 percent confidence interval: ## 79.08467 80.62748 ## sample estimates: ## mean of x ## 79.85607 t.test(df$chinese, mu = 78, alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: df$chinese ## t = 4.7742, df = 99, p-value = 3.122e-06 ## alternative hypothesis: true mean is greater than 78 ## 95 percent confidence interval: ## 79.21057 Inf ## sample estimates: ## mean of x ## 79.85607 t.test(df$chinese, mu = 78, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: df$chinese ## t = 4.7742, df = 99, p-value = 1 ## alternative hypothesis: true mean is less than 78 ## 95 percent confidence interval: ## -Inf 80.50158 ## sample estimates: ## mean of x ## 79.85607 wilcox.test(df$chinese, mu = 78)# skewed distributions ## ## Wilcoxon signed rank test with continuity correction ## ## data: df$chinese ## V = 3755, p-value = 2.364e-05 ## alternative hypothesis: true location is not equal to 78 6.1.3.1.3 Independent sample t-test t.test(class1,class2) ## ## Welch Two Sample t-test ## ## data: class1 and class2 ## t = 0.99214, df = 42.999, p-value = 0.3267 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.164602 3.420140 ## sample estimates: ## mean of x mean of y ## 80.25445 79.12668 6.1.3.1.4 Paired sample t-test t.test(df$chinese, df$math, paired = T) ## ## Paired t-test ## ## data: df$chinese and df$math ## t = 36.486, df = 99, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 19.19883 21.40712 ## sample estimates: ## mean of the differences ## 20.30298 6.1.3.1.5 wilcox.test wilcox.test(class1,class2)#not normally distributed ## ## Wilcoxon rank sum test ## ## data: class1 and class2 ## W = 359, p-value = 0.3753 ## alternative hypothesis: true location shift is not equal to 0 wilcox.test(df$chinese, df$math, paired = T) ## ## Wilcoxon signed rank test with continuity correction ## ## data: df$chinese and df$math ## V = 5050, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 6.1.3.2 ANOVA When the independent variable has more than two levels, we need to use ANOVA. We can use avo function in R or build a linear model. dif_class = aov (chinese ~ class, data = df) summary(dif_class) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## class 3 86.5 28.84 1.964 0.125 ## Residuals 96 1409.8 14.69 How to report: There is a significant difference among different classes in terms of their Chinese scores, F (3, 96) = 0.87, p = .46. After we run the model, it is very likely that we need to conduct pairwised comparison. However, we cannot simple use multiple t-tests but we need to correct the alpha value. There are different ways to do the correction: Bonferroni correction: a/n Tukey’s Honestly Significant difference: assuming the means for each level of the factor should be based on equal numbers of observation. #how is the difference like? TukeyHSD(dif_class) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = chinese ~ class, data = df) ## ## $class ## diff lwr upr p adj ## 2-1 -1.1277687 -3.9617063 1.706169 0.7261425 ## 3-1 -1.3981021 -4.2320397 1.435835 0.5715770 ## 4-1 0.9323559 -1.9015817 3.766293 0.8252544 ## 3-2 -0.2703334 -3.1042710 2.563604 0.9945172 ## 4-2 2.0601246 -0.7738130 4.894062 0.2346317 ## 4-3 2.3304580 -0.5034796 5.164396 0.1449208 # diff= the difference in the means # lwr = the lower end points of the confidence interval # p adj = adjusted p value plot(TukeyHSD(dif_class)) In the general linear model approach, residuals are normally distributed;groups should have the same variance, or homoscedasticity. Observations among groups are independent. That is, not paired or repeated measures data Moderate deviation from normally-distributed residuals is permissible #two-way ANOVA m1 &lt;- aov(chinese ~ sex + class, data = df) summary(m1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## sex 1 17.6 17.63 1.204 0.275 ## class 3 88.0 29.33 2.003 0.119 ## Residuals 95 1390.7 14.64 m2 &lt;- aov(chinese ~ sex * class, data = df) summary(m2) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## sex 1 17.6 17.63 1.243 0.268 ## class 3 88.0 29.33 2.067 0.110 ## sex:class 3 85.4 28.46 2.006 0.119 ## Residuals 92 1305.3 14.19 m3 &lt;- aov(chinese ~ sex + class + sex:class, data = df)# same as model2 summary(m3) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## sex 1 17.6 17.63 1.243 0.268 ## class 3 88.0 29.33 2.067 0.110 ## sex:class 3 85.4 28.46 2.006 0.119 ## Residuals 92 1305.3 14.19 TukeyHSD(m1) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = chinese ~ sex + class, data = df) ## ## $sex ## diff lwr upr p adj ## m-f 0.8425002 -0.6815228 2.366523 0.2752086 ## ## $class ## diff lwr upr p adj ## 2-1 -1.1951687 -4.0251528 1.634815 0.6877082 ## 3-1 -1.4655021 -4.2954863 1.364482 0.5310608 ## 4-1 0.8649559 -1.9650282 3.694940 0.8545808 ## 3-2 -0.2703334 -3.1003175 2.559651 0.9944911 ## 4-2 2.0601246 -0.7698595 4.890109 0.2334357 ## 4-3 2.3304580 -0.4995261 5.160442 0.1439839 TukeyHSD(m2) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = chinese ~ sex * class, data = df) ## ## $sex ## diff lwr upr p adj ## m-f 0.8425002 -0.6585127 2.343513 0.2678544 ## ## $class ## diff lwr upr p adj ## 2-1 -1.1951687 -3.9828745 1.592537 0.6770734 ## 3-1 -1.4655021 -4.2532079 1.322204 0.5177998 ## 4-1 0.8649559 -1.9227499 3.652662 0.8487133 ## 3-2 -0.2703334 -3.0580391 2.517372 0.9942296 ## 4-2 2.0601246 -0.7275811 4.847830 0.2212752 ## 4-3 2.3304580 -0.4572477 5.118164 0.1344067 ## ## $`sex:class` ## diff lwr upr p adj ## m:1-f:1 3.6361511 -1.041044 8.3133466 0.2483988 ## f:2-f:1 0.8006288 -3.985842 5.5870997 0.9995360 ## m:2-f:1 0.4737627 -4.026356 4.9738819 0.9999794 ## f:3-f:1 -0.4353037 -5.221775 4.3511672 0.9999925 ## m:3-f:1 0.9621143 -3.538005 5.4622334 0.9977287 ## f:4-f:1 3.3649557 -1.421515 8.1514266 0.3730334 ## m:4-f:1 2.1377284 -2.362391 6.6378475 0.8193905 ## f:2-m:1 -2.8355223 -7.712545 2.0415004 0.6194143 ## m:2-m:1 -3.1623884 -7.758704 1.4339274 0.4013818 ## f:3-m:1 -4.0714548 -8.948477 0.8055679 0.1732091 ## m:3-m:1 -2.6740368 -7.270353 1.9222790 0.6186399 ## f:4-m:1 -0.2711954 -5.148218 4.6058273 0.9999997 ## m:4-m:1 -1.4984227 -6.094738 3.0978930 0.9716476 ## m:2-f:2 -0.3268661 -5.034335 4.3806029 0.9999988 ## f:3-f:2 -1.2359325 -6.217849 3.7459844 0.9942610 ## m:3-f:2 0.1614855 -4.545983 4.8689545 1.0000000 ## f:4-f:2 2.5643269 -2.417590 7.5462438 0.7511349 ## m:4-f:2 1.3370996 -3.370369 6.0445685 0.9870814 ## f:3-m:2 -0.9090664 -5.616535 3.7984025 0.9988154 ## m:3-m:2 0.4883516 -3.927646 4.9043489 0.9999712 ## f:4-m:2 2.8911930 -1.816276 7.5986619 0.5510912 ## m:4-m:2 1.6639656 -2.752032 6.0799630 0.9387054 ## m:3-f:3 1.3974180 -3.310051 6.1048870 0.9833033 ## f:4-f:3 3.8002594 -1.181657 8.7821763 0.2705379 ## m:4-f:3 2.5730321 -2.134437 7.2805011 0.6901595 ## f:4-m:3 2.4028414 -2.304628 7.1103104 0.7589790 ## m:4-m:3 1.1756141 -3.240383 5.5916114 0.9912019 ## m:4-f:4 -1.2272273 -5.934696 3.4802417 0.9922431 6.1.3.3 Chisquare t-test When our data involves two categorical variables, and we want to know if these two variables are related or independent to each other, we can use the chisquare test. The null hypothesis of the independence assumption is to be rejected if the p-value of the following Chi-squared test statistics is less than a given significance level α. Here is an example: In the dataset survey, the Smoke column records the students smoking habit, while the Exer column records their exercise level. The allowed values in Smoke are “Heavy”, “Regul” (regularly), “Occas” (occasionally) and “Never”. As for Exer, they are “Freq” (frequently), “Some” and “None”. tbl = table(survey$Smoke, survey$Exer) chisq.test(tbl) ## Warning in chisq.test(tbl): Chi-squared approximation may be incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: tbl ## X-squared = 5.4885, df = 6, p-value = 0.4828 As the p-value 0.4828 is greater than the .05 significance level, we do not reject the null hypothesis that the smoking habit is independent of the exercise level of the students. "],
["parametric-tests-and-relevant-assumptions.html", "Chapter 7 Parametric tests and relevant assumptions 7.1 Parametric statistical tests 7.2 Assumptions 7.3 Assessing model assumptions", " Chapter 7 Parametric tests and relevant assumptions This section further illustrates assumptions of parametric tests and the methods to assess them. 7.1 Parametric statistical tests T-test, analysis of variance, and linear regression are all parametric statistical tests. They are used when the dependent variable is an interval/ratio data variable, such as length, height, weight. Advantages: your audience will likely be familiar with the techniques and interpretation of the results. These tests are also often more flexible and more powerful than their nonparametric analogues. Drawback: all parametric tests assume something about the distribution of the underlying data. If these assumptions are violated, the resultant test statistics will not be valid, and the tests will not be as powerful as for cases when assumptions are met. Count/categorical data may not be appropriate for common parametric tests. 7.2 Assumptions 7.2.1 Random sampling The data captured in the sample are randomly chosen from the population as a whole. Selection bias will obviously affect the validity of the outcome of the analysis. 7.2.2 Independent observations Tests will also assume that observations are independent of one another, except when the analysis takes non-independence into account. For example, in repeated measures experiments, the same subject is observed over time. Students with a high test score on one date to have a high test score on subsequent dates. In this case the observation on one date would not be independent of observations on other dates.The independence of observation is often assumed from good experimental design. Also, data or residuals can be plotted, for example to see if observations from one date are correlated to those for another date. 7.2.3 Normal distribution of data or residuals Parametric tests assume that the data come from a population of known distribution, such as normal distribution. That is, the data are normally distributed once the effects of the variables in the model are taken into account. Practically speaking, this means that the residuals from the analysis should be normally distributed. This will usually be assessed with a histogram of residuals, a density plot, or with quantile–quantile plot. A select number of tests (limited to one-sample t-test, two-sample t-test, and paired t-test) will require that data itself be normally distributed.For other tests, the distribution of the residuals will be investigated. Residuals, also commonly called errors, are the difference between the observations and the value predicted by the model. For example, if the calculated mean of a sample is 10, and one observation is 12, the residual for this observation is 2. Be careful not to get confused about this assumption. You may see discussion about how “data” should be normally distributed for parametric tests. This is usually wrong-headed. The t-test assumes that the observations for each group are normally distributed, but if there is a difference in the groups, we might expect a bi-modal distribution, not a simple normal distribution, for the combined data. This is why in most cases we look at the distribution of the residuals, not the raw data. 7.2.4 Homogeneity of variance Parametric analyses will also assume a homogeneity of variance among groups. That is, for Student’s t-test comparing two groups, each group should have the same variance.Homogeneity of variance is also called homoscedasticity. 7.2.5 Additivity of treatment effects Models for two-way analysis of variance and similar analyses are constructed as linear models in which the dependent variable is predicted as a linear combination of the independent variables. A violation of this assumption is sometimes indicated when a plot of residuals versus predicted values exhibits a curved pattern. 7.2.6 Outliers Outliers are observations whose value is far outside what is expected. They can play havoc with parametric analyses since they affect the distribution of the data and strongly influence the mean. There are a variety of formal tests for detecting outliers, but they will not be discussed here. The best approach is one that looks at residuals after an analysis. Good tools are the “Residuals vs. leverage” plot and other plots in the “Other diagnostic plots” section below. Some parametric tests are somewhat robust to violations of certain assumptions. For example, the t-test is reasonably robust to violations of normality for symmetric distributions, but not to samples having unequal variances (unless Welch’s t-test is used). A one-way analysis of variance is likewise reasonably robust to violations in normality. …model assumptions should always be checked, but you may be able to tolerate small violations in the distribution of residuals or homoscedasticity. Large violations will make the test invalid, though. It is important to be honest with your assessments when checking model assumptions. It is better to transform data, change your model, use a robust method, or use a nonparametric test than to not have confidence in your analysis. 7.3 Assessing model assumptions 7.3.1 normality of residuals There are formal tests to assess the normality of residuals. Common tests include Shapiro-Wilk Anderson–Darling Kolmogorov–Smirnov D’Agostino–Pearson However, their results are dependent on sample size. When the sample size is large, the tests may indicate a statistically significant departure from normality, even if that departure is small. And when sample sizes are small, they won’t detect departures from normality. In each case, the null hypothesis is that the data distribution is not different from normal. That is, a significant p-value (p &lt; 0.05) suggests that data are not normally distributed. library(tidyverse) # head(diamonds) # summary(diamonds) test_data = diamonds%&gt;% filter(cut %in% c(&quot;Fair&quot;, &quot;Ideal&quot; ), carat == 0.7, color %in% c(&quot;G&quot;, &quot;F&quot; ), clarity %in% c(&quot;SI1&quot;, &quot;VS2&quot; )) # table(test_data$cut) ggplot(test_data, aes(price, fill = cut)) + geom_density(position=&quot;dodge&quot;, alpha = 0.6) ## Warning: Width not defined. Set with `position_dodge(width = ?)` #Define a linear model model = lm(price ~ cut + color, data = test_data) #Shapiro–Wilk normality test x = residuals(model) shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.96048, p-value = 0.0006383 # Anderson-Darling normality test if(!require(nortest)){install.packages(&quot;nortest&quot;)} ## Loading required package: nortest library(nortest) x = residuals(model) ad.test(x) ## ## Anderson-Darling normality test ## ## data: x ## A = 1.7123, p-value = 0.0002071 # One-sample Kolmogorov-Smirnov test x = residuals(model) ks.test(x, &quot;pnorm&quot;, mean = mean(x), sd = sd(x)) ## Warning in ks.test(x, &quot;pnorm&quot;, mean = mean(x), sd = sd(x)): ties should not ## be present for the Kolmogorov-Smirnov test ## ## One-sample Kolmogorov-Smirnov test ## ## data: x ## D = 0.096286, p-value = 0.1666 ## alternative hypothesis: two-sided # D&#39;Agostino Normality Test if(!require(fBasics)){install.packages(&quot;fBasics&quot;)} ## Loading required package: fBasics ## Loading required package: timeDate ## ## Attaching package: &#39;timeDate&#39; ## The following objects are masked from &#39;package:PerformanceAnalytics&#39;: ## ## kurtosis, skewness ## Loading required package: timeSeries ## ## Attaching package: &#39;timeSeries&#39; ## The following object is masked from &#39;package:zoo&#39;: ## ## time&lt;- ## The following object is masked from &#39;package:psych&#39;: ## ## outlier ## ## Attaching package: &#39;fBasics&#39; ## The following object is masked from &#39;package:car&#39;: ## ## densityPlot ## The following object is masked from &#39;package:psych&#39;: ## ## tr library(fBasics) x = residuals(model) dagoTest(x) ## ## Title: ## D&#39;Agostino Normality Test ## ## Test Results: ## STATISTIC: ## Chi2 | Omnibus: 10.2249 ## Z3 | Skewness: 3.1412 ## Z4 | Kurtosis: 0.5981 ## P VALUE: ## Omnibus Test: 0.006021 ## Skewness Test: 0.001683 ## Kurtosis Test: 0.5498 ## ## Description: ## Sun Dec 01 18:18:54 2019 by user: Juqiang Chen 7.3.2 Skew and kurtosis There are no definitive guidelines as to what range of skew or kurtosis are acceptable for considering residuals to be normally distributed. we can rely on skew and kurtosis calculations,or histograms and other plots. If the absolute value is &gt; 0.5, BE CAUTIOUS If the absolute value is &gt; 1.0, consider it not normally distributed. Some authors use 2.0 as a cutoff for normality, and others use a higher limit for kurtosis. # library(psych) x = residuals(model) describe(x, type=2) ## vars n mean sd median trimmed mad min max range skew ## X1 1 134 0 284.79 -40.84 -20.5 262.42 -543.69 839.01 1382.7 0.7 ## kurtosis se ## X1 0.18 24.6 7.3.3 Visual inspection to assess the normality of residuals Usually, the best method to see if model residuals meet the assumptions of normal distribution and homoscedasticity are to plot them and inspect the plots visually. 7.3.3.1 Histogram with normal curve A histogram of the residuals should be approximately normal, without excessive skew or kurtosis. Adding a normal curve with the same mean and standard deviation as the data helps to assess the histogram. x = residuals(model) library(rcompanion) plotNormalHistogram(residuals(model)) 7.3.3.2 Kernel density plot with normal curve A kernel density plot is similar to a histogram, but is smoothed into a curve. Sometimes a density plot gives a better representation of the distribution of data, because the appearance of the histogram depends upon how many bins are used. The plotNormalDensity function will produce this plot. Options include those for the plot function, as well as adjust, bw, and kernel which are passed to the density function. col1, col2, and col3 change plot colors, and lwd changes line thickness. library(rcompanion) x = residuals(model) plotNormalDensity(x, adjust = 1) ### Decrease this number ### to make line less smooth 7.3.4 Visual inspection for homogeneity of variance Patterns in the plot of residuals versus fitted values can indicate a lack of homoscedasticity or that errors are not independent of fitted values. plot(fitted(model), residuals(model)) In the four plots below, A) Residuals.a show normally distributed and homoscedastic residuals, suggesting model assumptions were met. B) Residuals.b show a non-normal distribution of residuals. C) Residuals.c show that the residuals are not independent of the fitted values. In this case, the model needs to be modified in order to describe the data well. D) Residuals.d show heteroscedasticity, since variability in the residuals is greater for large fitted values than for small fitted values. (Adapted from similar plots in Tabachnick, 2001). (#fig:residual plot)residual plot 7.3.5 formal tests for homogeneity of variance In each case, the null hypothesis is that the variance among groups is not different. That is, a significant p-value (p &lt; 0.05) suggests that the variance among groups is different. #Define a linear model model = lm(price ~ cut + color, data = test_data) 7.3.5.1 Bartlett’s test for homogeneity of variance Bartlett’s test is known to be sensitive to non-normality in samples. That is, non-normal samples can result in a significant test due to the non-normality. x = residuals(model) bartlett.test(x ~ interaction(cut, color), data = test_data) ## ## Bartlett test of homogeneity of variances ## ## data: x by interaction(cut, color) ## Bartlett&#39;s K-squared = 39.068, df = 3, p-value = 1.679e-08 7.3.5.2 Levene’s test for homogeneity of variance Levene’s test is an alternative to Bartlett’s that is supposedly less sensitive to departures from normality in the data. #library(car) x = residuals(model) leveneTest(x ~ cut * color, data=test_data, center=mean) ### original Levene’s test ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 3 8.76 2.468e-05 *** ## 130 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.3.5.3 Brown–Forsythe or robust Levene’s test The Brown–Forsythe modification of Levene’s test makes it more robust to departures in normality of the data. x = residuals(model) #library(car) leveneTest(x ~ cut * color, data=test_data) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 8.5541 3.163e-05 *** ## 130 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #library(lawstat) levene.test(x, interaction(test_data$cut, test_data$color)) ## ## Modified robust Brown-Forsythe Levene-type test based on the ## absolute deviations from the median ## ## data: x ## Test Statistic = 0.51854, p-value = 0.6702 7.3.5.4 Fligner-Killeen test The Fligner-Killeen test is another test for homogeneity of variances that is robust to departures in normality of the data. x = residuals(model) fligner.test(x ~ interaction(cut, color), data=test_data) ## ## Fligner-Killeen test of homogeneity of variances ## ## data: x by interaction(cut, color) ## Fligner-Killeen:med chi-squared = 11.948, df = 3, p-value = ## 0.007562 References: https://rcompanion.org/handbook/I_01.html "],
["resources.html", "Chapter 8 Resources", " Chapter 8 Resources Choose the best graph for data presentation http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/DataPresentation/DataPresentation7.html#headingtaglink_6 "]
]
