[
["index.html", "A Introduction to Eploratory Data Analysis with R Chapter 1 Prerequisites 1.1 Installing R and R studio 1.2 Install and library packages 1.3 Programme", " A Introduction to Eploratory Data Analysis with R Juqiang Chen 2019-10-28 Chapter 1 Prerequisites This document is to accompany Eploratory Data Analysis with R tutorial for DH Downunder 2019 at the University of Newcastle, Australia, from 9-13 December. I am a speech scientist working on cross-language lexical tone perception and production. I have rich experience dealing with experimental data and I am keen to help others with data wrangling, data visualization and statistical modelling problems. I aspire to promote a streamlined workflow with R packages to improve data analysis efficiency in quantitative analysis in the field of social science and linguistics. If you have any questions about the tutorial, please e-mail me at: j.chen2@westernsydney.edu.au This workshop will show how to use data transformation and visualization to explore your data in a systematic way, or in a statistical term, exploratory data analysis. Participants will learn to generate questions about the data, search for answers by transforming, visualizing and modeling the dataset, and use what they learn to further refine the questions and/or generate new questions. The workshop will start by exploring variations in (categorical and continuous) one variable and move on to investigate covariations among two or three variables. Participants will learn to produce summary tables (calculating mean or standard deviation etc. of one or multiple variables by one or more variables) and will also learn to draw figures with ggplot2. This workshop builds on some knowledge of data wrangling. Therefore, it is desirable that participants should take the Introduction to data wrangling with R, if they have no such knowledge. Participants are welcomed to bring their own data and apply what they learn on the spot. Before we start our journey of data wrangling with R, you will need to install R on your laptop. R is multi-platform, which means you can install R on your PC or MAC. 1.1 Installing R and R studio Use this link [https://cloud.r-project.org/] to download R and select the proper version for your laptop. knitr::include_graphics(&quot;img/installr.jpg&quot;) Figure 1.1: Download R RStudio is an integrated development environment, or IDE, for R programming. Download and install it from [http://www.rstudio.com/download.] The free version is poweful enough. 1.2 Install and library packages install.packages(“package_name”) library(package_name) 1.3 Programme Session 1 chapter 1 &amp; 2 Session 2 chapter 3, 4 &amp; 5 Session 3 chapter 3, 4 &amp; 5 "],
["intro.html", "Chapter 2 Basic data structures in R 2.1 Categorical vs. numeric variables 2.2 1D data structure: vectors 2.3 2D data structures: matrice and data frames 2.4 summary", " Chapter 2 Basic data structures in R Before we get our hands dirty in doing actual data analysis, it is desirable to first think about what types of variables and data structures we are dealing with. A variable is any characteristics, number, or quantity that can be measured or counted. A variable may also be called a data item. Age, sex, business income and expenses, country of birth, capital expenditure, class grades, eye colour and vehicle type are examples of variables. It is called a variable because the value may vary between data units in a population, and may change in value over time.(Australian Bureau of Statistics, ABS) Before we talk about data structures in R, let’s first think about how data can be categorized. 2.1 Categorical vs. numeric variables Categorical variables have values that describe a ‘quality’ or ‘characteristic’ of a data unit, like ‘what type’ or ‘which category’. Categorical variables fall into mutually exclusive (in one category or in another) and exhaustive (include all possible options) categories. Therefore, categorical variables are qualitative variables and tend to be represented by a non-numeric value. Categorical variables may be further described as ordinal or nominal: An ordinal variable is a categorical variable. Observations can take a value that can be logically ordered or ranked. The categories associated with ordinal variables can be ranked higher or lower than another, but do not necessarily establish a numeric difference between each category.In other words, the interval between levels of the variables are unknown. Examples of ordinal categorical variables include academic grades (i.e. A, B, C), clothing size (i.e. small, medium, large, extra large) and attitudes (i.e. strongly agree, agree, disagree, strongly disagree). For example, when doing a survey, participants will be asked to rate. The subjective measurements of this kind are often ordinal variables. E.g. a Likert ranking scale; level of education (“&lt; high school”, “high school”, “associate’s degree”). We can assign numbers to levels of an ordinal variable, and can order them, but we should bear in mind that these variable are not numeric. For example, “strongly agree” and “neutral” cannot average out to an “agree.”, even though you can assign 5 to “strong agree” and 3 to “neutral”. A nominal variable is a categorical variable. Observations can take a value that is not able to be organised in a logical sequence. Examples of nominal categorical variables include sex, business type, eye colour, religion and brand. The data collected for a categorical variable are qualitative data. Numeric variables have values that describe a measurable quantity as a number, like ‘how many’ or ‘how much’. Therefore numeric variables are quantitative variables.(ABS) It is also called Interval/ratio variables and the interval between numbers is equal: the interval between 1 kg and 2 kg is the same as between 3 kg and 4 kg. Numeric variables may be further described as either continuous or discrete: A continuous variable is a numeric variable. Observations can take any value between a certain set of real numbers. The value given to an observation for a continuous variable can include values as small as the instrument of measurement allows. Examples of continuous variables include height, time, age, and temperature.(ABS) A discrete variable is a numeric variable. Observations can take a value based on a count from a set of distinct whole values. A discrete variable cannot take the value of a fraction between one value and the next closest value. Examples of discrete variables include the number of registered cars, number of business locations, and number of children in a family, all of of which measured as whole units (i.e. 1, 2, 3 cars).(ABS) The data collected for a numeric variable are quantitative data. The variable type will determine (1) statistical analysis; (2) the way we summarize data with statistics and plots. knitr::include_graphics(&quot;img/variable.jpg&quot;) (#fig:variable_type)Download R Variables can be stored in R in different data types. Normial and ordinal variables can be stored as character or factors (with levels). Interval data are stored as numbers either as integer or numeric (real or decimal). If you have only one variable, you can store it in a vector. However, more often than not, you have a bunch of variables that should be stored or imported as a matrix or data frame. 2.2 1D data structure: vectors A vector is a sequence of data elements of the same basic type: integer, double, logical or character. All elements of a vector must be the same type. 2.2.1 Creating vectors a = 8:17 b &lt;- c(9, 10, 100, 38) c = c (TRUE, FALSE, TRUE, FALSE) c = c (T, F, T, F) d = c (&quot;TRUE&quot;, &quot;FALSE&quot;, &quot;FALSE&quot;) # You can change the type of a vector with as.vector function. as.vector(b, mode = &quot;character&quot;) ## [1] &quot;9&quot; &quot;10&quot; &quot;100&quot; &quot;38&quot; # When you put elements of different types in one vector, R will automatically change the type of some elements to keep the whole vector homogenous. e = c(9,10, &quot;ab&quot;, &quot;cd&quot;) f = c(10, 11, T, F) c () is a function in R. There are some other basic functions in R that you can play with to generate vectors. A = 9:20 + 1 B = seq (1, 10) C = seq (1, 20, by= 2) D = rep (5, 4) E = rep (c(1,2,3), 4) G = rep (c(1,2,3), each = 4) # Now that you have a vector, you can do some Maths. max(a) ## [1] 17 min(a) ## [1] 8 range(a) ## [1] 8 17 sum(a) ## [1] 125 mean(a) ## [1] 12.5 median(a) ## [1] 12.5 quantile(a) ## 0% 25% 50% 75% 100% ## 8.00 10.25 12.50 14.75 17.00 sd(a) ## [1] 3.02765 round(sd(a), 2) ## [1] 3.03 2.2.2 creating list objects We can put vectors of different types (e.g., number, logic or character) and lengths in a list object. list1 = list(a, b, c, d, e, f) list1 ## [[1]] ## [1] 8 9 10 11 12 13 14 15 16 17 ## ## [[2]] ## [1] 9 10 100 38 ## ## [[3]] ## [1] TRUE FALSE TRUE FALSE ## ## [[4]] ## [1] &quot;TRUE&quot; &quot;FALSE&quot; &quot;FALSE&quot; ## ## [[5]] ## [1] &quot;9&quot; &quot;10&quot; &quot;ab&quot; &quot;cd&quot; ## ## [[6]] ## [1] 10 11 1 0 # More often than not, we do not make list ourselves but have to deal with lists when we get outputs from stats models. 2.3 2D data structures: matrice and data frames Most of us have had some experience with the Excel spreadsheet. Data in a spreadsheet are arranged by rows and columns in a rectangular space. This is a typical 2 dimensional data structure. In R, we can have two ways of forming tabular data like a spreadsheet: the matrix and dataframe. A matrix is a collection of data elements arranged in a two-dimensional rectangular layout in which all the elements must be of the same type (e.g., numeric or character). Dataframe is similar to matrix in shape but only differs in that different types of data can co-exist in different columns. Thus, in data analysis, we use dataframes more often than matrix. # Let&#39;s generate a dataframe from scratch. id = seq(1, 40) gender = rep(c(&quot;male&quot;, &quot;female&quot;), 5) maths = rnorm(40, mean = 70, sd = 5) english = rnorm(40, mean = 80, sd = 9) music = rnorm(40, mean = 75, sd = 10) pe = rnorm(40, mean = 86, sd = 12) df1 = data.frame (id, gender, maths, english) Now let’s explore the data frame we just created. str(df1) ## &#39;data.frame&#39;: 40 obs. of 4 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ gender : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 1 2 1 2 1 2 1 ... ## $ maths : num 68.4 64.8 70.7 74.2 66.8 ... ## $ english: num 73.7 81.8 62.7 69.4 68.1 ... summary(df1) ## id gender maths english ## Min. : 1.00 female:20 Min. :61.42 Min. :56.16 ## 1st Qu.:10.75 male :20 1st Qu.:68.94 1st Qu.:70.95 ## Median :20.50 Median :71.04 Median :75.72 ## Mean :20.50 Mean :71.19 Mean :76.58 ## 3rd Qu.:30.25 3rd Qu.:73.92 3rd Qu.:82.41 ## Max. :40.00 Max. :86.32 Max. :95.90 nrow(df1) ## [1] 40 ncol(df1) ## [1] 4 attributes(df1) ## $names ## [1] &quot;id&quot; &quot;gender&quot; &quot;maths&quot; &quot;english&quot; ## ## $class ## [1] &quot;data.frame&quot; ## ## $row.names ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## [24] 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 2.3.1 what if I want to change column names or add variable to the df? df2 = data.frame (id = id, gender = gender, maths = maths, english = english) df2 = cbind(df2, pe) colnames(df2) = c(&quot;ID&quot;, &quot;SEX&quot;,&quot;MATHS&quot;,&quot;ENGLISH&quot;,&quot;PE&quot;) head(df2) ## ID SEX MATHS ENGLISH PE ## 1 1 male 68.37890 73.65576 101.87517 ## 2 2 female 64.76099 81.79780 96.67504 ## 3 3 male 70.66136 62.67545 89.94748 ## 4 4 female 74.18511 69.36135 101.49803 ## 5 5 male 66.76948 68.06133 95.49593 ## 6 6 female 77.32655 72.28366 81.70524 2.3.2 Subsetting dataframes We all know how to select part of an Excel spreadsheet by clicking and moving our mouse. In R, when we want to select part of a dataframe, we use this formula, dataframe[row, column]. There are various ways we can use this formula and believe it or not, you will love them! # the complete dataset df2 ## ID SEX MATHS ENGLISH PE ## 1 1 male 68.37890 73.65576 101.87517 ## 2 2 female 64.76099 81.79780 96.67504 ## 3 3 male 70.66136 62.67545 89.94748 ## 4 4 female 74.18511 69.36135 101.49803 ## 5 5 male 66.76948 68.06133 95.49593 ## 6 6 female 77.32655 72.28366 81.70524 ## 7 7 male 71.14122 82.14418 88.27962 ## 8 8 female 70.23516 75.46044 73.46997 ## 9 9 male 67.11613 88.67836 84.53432 ## 10 10 female 62.42870 80.37444 89.31138 ## 11 11 male 65.83156 83.11145 92.33215 ## 12 12 female 71.42700 77.94350 72.81225 ## 13 13 male 76.87431 84.06092 91.04300 ## 14 14 female 72.89258 88.98224 80.50066 ## 15 15 male 72.79514 74.57316 82.20769 ## 16 16 female 70.56161 56.16341 75.69889 ## 17 17 male 70.93129 95.90447 66.80945 ## 18 18 female 73.68635 86.45945 99.13487 ## 19 19 male 71.92747 76.71519 83.18177 ## 20 20 female 76.54937 70.11313 86.91269 ## 21 21 male 69.57280 83.75215 90.75758 ## 22 22 female 74.28147 69.35617 90.27291 ## 23 23 male 63.53660 81.76192 111.08221 ## 24 24 female 70.86696 84.09883 99.31830 ## 25 25 male 73.83766 74.51313 91.14586 ## 26 26 female 69.99588 75.79803 101.51540 ## 27 27 male 69.87460 74.61607 100.43810 ## 28 28 female 86.31795 82.18142 92.82180 ## 29 29 male 61.42393 70.82165 97.35876 ## 30 30 female 73.09703 70.99206 92.25140 ## 31 31 male 63.41163 86.40465 101.47166 ## 32 32 female 75.37470 66.82335 62.99479 ## 33 33 male 72.13253 73.26800 99.36721 ## 34 34 female 67.98527 65.76944 77.18518 ## 35 35 male 74.53076 75.64522 105.28903 ## 36 36 female 78.20103 83.82450 105.89472 ## 37 37 male 69.97977 75.16115 76.95511 ## 38 38 female 76.17259 76.67328 86.47751 ## 39 39 male 71.52254 76.12839 56.56979 ## 40 40 female 69.12040 67.05608 67.26367 df2[2:5, ] # from row 2 to row 5 ## ID SEX MATHS ENGLISH PE ## 2 2 female 64.76099 81.79780 96.67504 ## 3 3 male 70.66136 62.67545 89.94748 ## 4 4 female 74.18511 69.36135 101.49803 ## 5 5 male 66.76948 68.06133 95.49593 df2[ , 1:2] # select column 1 to 2 ## ID SEX ## 1 1 male ## 2 2 female ## 3 3 male ## 4 4 female ## 5 5 male ## 6 6 female ## 7 7 male ## 8 8 female ## 9 9 male ## 10 10 female ## 11 11 male ## 12 12 female ## 13 13 male ## 14 14 female ## 15 15 male ## 16 16 female ## 17 17 male ## 18 18 female ## 19 19 male ## 20 20 female ## 21 21 male ## 22 22 female ## 23 23 male ## 24 24 female ## 25 25 male ## 26 26 female ## 27 27 male ## 28 28 female ## 29 29 male ## 30 30 female ## 31 31 male ## 32 32 female ## 33 33 male ## 34 34 female ## 35 35 male ## 36 36 female ## 37 37 male ## 38 38 female ## 39 39 male ## 40 40 female df2[ , c(&quot;ENGLISH&quot;, &quot;PE&quot;)] # select by column names ## ENGLISH PE ## 1 73.65576 101.87517 ## 2 81.79780 96.67504 ## 3 62.67545 89.94748 ## 4 69.36135 101.49803 ## 5 68.06133 95.49593 ## 6 72.28366 81.70524 ## 7 82.14418 88.27962 ## 8 75.46044 73.46997 ## 9 88.67836 84.53432 ## 10 80.37444 89.31138 ## 11 83.11145 92.33215 ## 12 77.94350 72.81225 ## 13 84.06092 91.04300 ## 14 88.98224 80.50066 ## 15 74.57316 82.20769 ## 16 56.16341 75.69889 ## 17 95.90447 66.80945 ## 18 86.45945 99.13487 ## 19 76.71519 83.18177 ## 20 70.11313 86.91269 ## 21 83.75215 90.75758 ## 22 69.35617 90.27291 ## 23 81.76192 111.08221 ## 24 84.09883 99.31830 ## 25 74.51313 91.14586 ## 26 75.79803 101.51540 ## 27 74.61607 100.43810 ## 28 82.18142 92.82180 ## 29 70.82165 97.35876 ## 30 70.99206 92.25140 ## 31 86.40465 101.47166 ## 32 66.82335 62.99479 ## 33 73.26800 99.36721 ## 34 65.76944 77.18518 ## 35 75.64522 105.28903 ## 36 83.82450 105.89472 ## 37 75.16115 76.95511 ## 38 76.67328 86.47751 ## 39 76.12839 56.56979 ## 40 67.05608 67.26367 df2[c(1,2,3), ] #select the first three rows ## ID SEX MATHS ENGLISH PE ## 1 1 male 68.37890 73.65576 101.87517 ## 2 2 female 64.76099 81.79780 96.67504 ## 3 3 male 70.66136 62.67545 89.94748 df2[seq(1, 40, 2), ] #select every other rows from 1 to 40 rows ## ID SEX MATHS ENGLISH PE ## 1 1 male 68.37890 73.65576 101.87517 ## 3 3 male 70.66136 62.67545 89.94748 ## 5 5 male 66.76948 68.06133 95.49593 ## 7 7 male 71.14122 82.14418 88.27962 ## 9 9 male 67.11613 88.67836 84.53432 ## 11 11 male 65.83156 83.11145 92.33215 ## 13 13 male 76.87431 84.06092 91.04300 ## 15 15 male 72.79514 74.57316 82.20769 ## 17 17 male 70.93129 95.90447 66.80945 ## 19 19 male 71.92747 76.71519 83.18177 ## 21 21 male 69.57280 83.75215 90.75758 ## 23 23 male 63.53660 81.76192 111.08221 ## 25 25 male 73.83766 74.51313 91.14586 ## 27 27 male 69.87460 74.61607 100.43810 ## 29 29 male 61.42393 70.82165 97.35876 ## 31 31 male 63.41163 86.40465 101.47166 ## 33 33 male 72.13253 73.26800 99.36721 ## 35 35 male 74.53076 75.64522 105.28903 ## 37 37 male 69.97977 75.16115 76.95511 ## 39 39 male 71.52254 76.12839 56.56979 2.4 summary Dimensions Homogenous Heterogeneous 1D Atomic Vector List 2D Matrix Data frame nD Array "],
["what-is-eda.html", "Chapter 3 What is EDA? 3.1 Definition (from Wikipedia) 3.2 The objectives of EDA 3.3 Data analysis workflow", " Chapter 3 What is EDA? 3.1 Definition (from Wikipedia) In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. Exploratory data analysis was promoted by John Tukey to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA),which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA. 3.2 The objectives of EDA – To Suggest hypotheses about the causes of observed phenomena – To Assess assumptions on which statistical inference will be based – To Support the selection of appropriate statistical tools and techniques – To Provide a basis for further data collection through surveys or experiments 3.3 Data analysis workflow A common workflow for data analysis involves importing data, cleaning data, transforming data, visualizing and modeling data for reports or papers. However, you may notice that it is not a linear process. In other words, there is no magic way of understanding your data with a touch. Rather, it is a process where you need to try different wayd of tranforming, visualizing and modeling your data. Therefore, EDA (within the grey square)is not a formal process with a strict set of rules, but an open process. Fortunately, R offers a range of tools that can help us eaily summarize, visualize and model our data. In this workshop, we first explore variation within a variable and then move on to covariation among variables. knitr::include_graphics(&quot;img/DA_workflow.png&quot;) Figure 3.1: Data analysis workflow "],
["variation.html", "Chapter 4 Variation 4.1 Categorical variable 4.2 Continous variable", " Chapter 4 Variation Variation is the tendency of the values of a variable to change from measurement to measurement. (Wickham &amp; Grolemund) 4.1 Categorical variable The best way to characterizing categorical variables is via the frequency. The frequency is the number of times a particular value for a variable (data item) has been observed to occur. How can we measure frequency? The frequency of a value can be expressed in different ways, depending on the purpose required. The absolute frequency describes the number of times a particular value for a variable (data item) has been observed to occur. Or simply put, counts. A relative frequency describes the number of times a particular value for a variable (data item) has been observed to occur in relation to the total number of values for that variable.It is calculated by dividing the absolute frequency by the total number of values for the variable. Ratios, rates, proportions and percentages are different ways of expressing relative frequencies. A ratiocompares the frequency of one value for a variable with another value for the variable. The first value identified in a ratio must be to the left of the colon (:) and the second value must be to the right of the colon (1st value : 2nd value). For example, in a total of 20 coin tosses where there are 12 heads and 8 tails, the ratio of heads to tails is 12:8. Alternatively, the ratio of tails to heads is 8:12. A rate is a measurement of one value for a variable in relation to another measured quantity. For example, in a total of 20 coin tosses where there are 12 heads and 8 tails, the rate is 12 heads per 20 coin tosses. Alternatively, the rate is 8 tails per 20 coin tosses. A proportion describes the share of one value for a variable in relation to a whole.It is calculated by dividing the number of times a particular value for a variable has been observed, by the total number of values in the population. For example, in a total of 20 coin tosses where there are 12 heads and 8 tails, the proportion of heads is 0.6 (12 divided by 20). Alternatively, the proportion of tails is 0.4 (8 divided by 20). A percentage expresses a value for a variable in relation to a whole population as a fraction of one hundred. The percentage total of an entire dataset should always add up to 100, as 100% represents the total, it is equal to the ‘whole’. A percentage is calculated by dividing the number of times a particular value for a variable has been observed, by the total number of observations in the population, then multiplying this number by 100. For example, in a total of 20 coin tosses where there are 12 heads and 8 tails, the percentage of heads is 60% (12 divided by 20, multiplied by 100). Alternatively, the percentage of tails is 40% (8 divided by 20, multiplied by 100). (ABS) Frequency distributions are visual displays that organise and present frequency counts so that the information can be interpreted more easily. How do we show a frequency distribution? A frequency distribution of data can be shown in a table or graph. Some common methods of showing frequency distributions include frequency tables, bar charts or histograms. Frequency Tables A frequency table is a simple way to display the number of occurrences of a particular value or characteristic. library(tidyverse) ## -- Attaching packages ---------------- ## v ggplot2 3.1.0 v purrr 0.3.2 ## v tibble 2.1.1 v dplyr 0.8.0.1 ## v tidyr 0.8.3 v stringr 1.4.0 ## v readr 1.3.1 v forcats 0.4.0 ## -- Conflicts ------------------------- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() summary(diamonds) ## carat cut color clarity ## Min. :0.2000 Fair : 1610 D: 6775 SI1 :13065 ## 1st Qu.:0.4000 Good : 4906 E: 9797 VS2 :12258 ## Median :0.7000 Very Good:12082 F: 9542 SI2 : 9194 ## Mean :0.7979 Premium :13791 G:11292 VS1 : 8171 ## 3rd Qu.:1.0400 Ideal :21551 H: 8304 VVS2 : 5066 ## Max. :5.0100 I: 5422 VVS1 : 3655 ## J: 2808 (Other): 2531 ## depth table price x ## Min. :43.00 Min. :43.00 Min. : 326 Min. : 0.000 ## 1st Qu.:61.00 1st Qu.:56.00 1st Qu.: 950 1st Qu.: 4.710 ## Median :61.80 Median :57.00 Median : 2401 Median : 5.700 ## Mean :61.75 Mean :57.46 Mean : 3933 Mean : 5.731 ## 3rd Qu.:62.50 3rd Qu.:59.00 3rd Qu.: 5324 3rd Qu.: 6.540 ## Max. :79.00 Max. :95.00 Max. :18823 Max. :10.740 ## ## y z ## Min. : 0.000 Min. : 0.000 ## 1st Qu.: 4.720 1st Qu.: 2.910 ## Median : 5.710 Median : 3.530 ## Mean : 5.735 Mean : 3.539 ## 3rd Qu.: 6.540 3rd Qu.: 4.040 ## Max. :58.900 Max. :31.800 ## str(diamonds) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 53940 obs. of 10 variables: ## $ carat : num 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... ## $ cut : Ord.factor w/ 5 levels &quot;Fair&quot;&lt;&quot;Good&quot;&lt;..: 5 4 2 4 2 3 3 3 1 3 ... ## $ color : Ord.factor w/ 7 levels &quot;D&quot;&lt;&quot;E&quot;&lt;&quot;F&quot;&lt;&quot;G&quot;&lt;..: 2 2 2 6 7 7 6 5 2 5 ... ## $ clarity: Ord.factor w/ 8 levels &quot;I1&quot;&lt;&quot;SI2&quot;&lt;&quot;SI1&quot;&lt;..: 2 3 5 4 2 6 7 3 4 5 ... ## $ depth : num 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... ## $ table : num 55 61 65 58 58 57 57 55 61 61 ... ## $ price : int 326 326 327 334 335 336 336 337 337 338 ... ## $ x : num 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... ## $ y : num 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... ## $ z : num 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... library(Hmisc) ## Loading required package: lattice ## Loading required package: survival ## Loading required package: Formula ## ## Attaching package: &#39;Hmisc&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## src, summarize ## The following objects are masked from &#39;package:base&#39;: ## ## format.pval, units describe(diamonds) ## diamonds ## ## 10 Variables 53940 Observations ## --------------------------------------------------------------------------- ## carat ## n missing distinct Info Mean Gmd .05 .10 ## 53940 0 273 0.999 0.7979 0.5122 0.30 0.31 ## .25 .50 .75 .90 .95 ## 0.40 0.70 1.04 1.51 1.70 ## ## lowest : 0.20 0.21 0.22 0.23 0.24, highest: 4.00 4.01 4.13 4.50 5.01 ## --------------------------------------------------------------------------- ## cut ## n missing distinct ## 53940 0 5 ## ## Value Fair Good Very Good Premium Ideal ## Frequency 1610 4906 12082 13791 21551 ## Proportion 0.030 0.091 0.224 0.256 0.400 ## --------------------------------------------------------------------------- ## color ## n missing distinct ## 53940 0 7 ## ## Value D E F G H I J ## Frequency 6775 9797 9542 11292 8304 5422 2808 ## Proportion 0.126 0.182 0.177 0.209 0.154 0.101 0.052 ## --------------------------------------------------------------------------- ## clarity ## n missing distinct ## 53940 0 8 ## ## Value I1 SI2 SI1 VS2 VS1 VVS2 VVS1 IF ## Frequency 741 9194 13065 12258 8171 5066 3655 1790 ## Proportion 0.014 0.170 0.242 0.227 0.151 0.094 0.068 0.033 ## --------------------------------------------------------------------------- ## depth ## n missing distinct Info Mean Gmd .05 .10 ## 53940 0 184 0.999 61.75 1.515 59.3 60.0 ## .25 .50 .75 .90 .95 ## 61.0 61.8 62.5 63.3 63.8 ## ## lowest : 43.0 44.0 50.8 51.0 52.2, highest: 72.2 72.9 73.6 78.2 79.0 ## --------------------------------------------------------------------------- ## table ## n missing distinct Info Mean Gmd .05 .10 ## 53940 0 127 0.98 57.46 2.448 54 55 ## .25 .50 .75 .90 .95 ## 56 57 59 60 61 ## ## lowest : 43.0 44.0 49.0 50.0 50.1, highest: 71.0 73.0 76.0 79.0 95.0 ## --------------------------------------------------------------------------- ## price ## n missing distinct Info Mean Gmd .05 .10 ## 53940 0 11602 1 3933 4012 544 646 ## .25 .50 .75 .90 .95 ## 950 2401 5324 9821 13107 ## ## lowest : 326 327 334 335 336, highest: 18803 18804 18806 18818 18823 ## --------------------------------------------------------------------------- ## x ## n missing distinct Info Mean Gmd .05 .10 ## 53940 0 554 1 5.731 1.276 4.29 4.36 ## .25 .50 .75 .90 .95 ## 4.71 5.70 6.54 7.31 7.66 ## ## lowest : 0.00 3.73 3.74 3.76 3.77, highest: 10.01 10.02 10.14 10.23 10.74 ## --------------------------------------------------------------------------- ## y ## n missing distinct Info Mean Gmd .05 .10 ## 53940 0 552 1 5.735 1.269 4.30 4.36 ## .25 .50 .75 .90 .95 ## 4.72 5.71 6.54 7.30 7.65 ## ## Value 0.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 ## Frequency 7 5 1731 12305 7817 5994 6742 9260 4298 3402 ## Proportion 0.000 0.000 0.032 0.228 0.145 0.111 0.125 0.172 0.080 0.063 ## ## Value 8.0 8.5 9.0 9.5 10.0 10.5 32.0 59.0 ## Frequency 1635 652 69 14 6 1 1 1 ## Proportion 0.030 0.012 0.001 0.000 0.000 0.000 0.000 0.000 ## --------------------------------------------------------------------------- ## z ## n missing distinct Info Mean Gmd .05 .10 ## 53940 0 375 1 3.539 0.7901 2.65 2.69 ## .25 .50 .75 .90 .95 ## 2.91 3.53 4.04 4.52 4.73 ## ## Value 0.0 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 ## Frequency 20 1 2 3 8807 13809 9474 13682 5525 2352 ## Proportion 0.000 0.000 0.000 0.000 0.163 0.256 0.176 0.254 0.102 0.044 ## ## Value 5.5 6.0 6.5 7.0 8.0 32.0 ## Frequency 237 20 5 1 1 1 ## Proportion 0.004 0.000 0.000 0.000 0.000 0.000 ## --------------------------------------------------------------------------- contents(diamonds) ## ## Data frame:diamonds 53940 observations and 10 variables Maximum # NAs:0 ## ## ## Levels Class Storage ## carat double ## cut 5 ordered integer ## color 7 ordered integer ## clarity 8 ordered integer ## depth double ## table double ## price integer ## x double ## y double ## z double ## ## +--------+---------------------------------+ ## |Variable|Levels | ## +--------+---------------------------------+ ## | cut |Fair,Good,Very Good,Premium,Ideal| ## +--------+---------------------------------+ ## | color |D,E,F,G,H,I,J | ## +--------+---------------------------------+ ## | clarity|I1,SI2,SI1,VS2,VS1,VVS2,VVS1,IF | ## +--------+---------------------------------+ Categorical variables are usually stored as factors or characters. You can use count() function or bar chart to explore the distribution. A bar chart is a type of graph in which each column (plotted either vertically or horizontally) represents a categorical variable or a discrete ungrouped numeric variable. It is used to compare the frequency (count) for a category or characteristic with another category or characteristic.(ABS) How to interprate: In a bar chart, the bar height (if vertical) or length (if horizontal) shows the frequency for each category or characteristic. The distribution of the dataset is not important because the columns each represent an individual category or characteristic rather than intervals for a continuous measurement. Therefore, gaps are included between each bar and each bar can be arranged in any order without affecting the data. count(diamonds, cut) ## # A tibble: 5 x 2 ## cut n ## &lt;ord&gt; &lt;int&gt; ## 1 Fair 1610 ## 2 Good 4906 ## 3 Very Good 12082 ## 4 Premium 13791 ## 5 Ideal 21551 count(diamonds, color) ## # A tibble: 7 x 2 ## color n ## &lt;ord&gt; &lt;int&gt; ## 1 D 6775 ## 2 E 9797 ## 3 F 9542 ## 4 G 11292 ## 5 H 8304 ## 6 I 5422 ## 7 J 2808 ggplot(diamonds)+ geom_bar(aes(x = cut)) ggplot(diamonds)+ geom_bar(aes(x = color)) Try this with clarity! library(languageR) ## ## Attaching package: &#39;languageR&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## english summary(lexdec) ## Subject RT Trial Sex NativeLanguage ## A1 : 79 Min. :5.829 Min. : 23 F:1106 English:948 ## A2 : 79 1st Qu.:6.215 1st Qu.: 64 M: 553 Other :711 ## A3 : 79 Median :6.346 Median :106 ## C : 79 Mean :6.385 Mean :105 ## D : 79 3rd Qu.:6.502 3rd Qu.:146 ## I : 79 Max. :7.587 Max. :185 ## (Other):1185 ## Correct PrevType PrevCorrect Word ## correct :1594 nonword:855 correct :1542 almond : 21 ## incorrect: 65 word :804 incorrect: 117 ant : 21 ## apple : 21 ## apricot : 21 ## asparagus: 21 ## avocado : 21 ## (Other) :1533 ## Frequency FamilySize SynsetCount Length ## Min. :1.792 Min. :0.0000 Min. :0.6931 Min. : 3.000 ## 1st Qu.:3.951 1st Qu.:0.0000 1st Qu.:1.0986 1st Qu.: 5.000 ## Median :4.754 Median :0.0000 Median :1.0986 Median : 6.000 ## Mean :4.751 Mean :0.7028 Mean :1.3154 Mean : 5.911 ## 3rd Qu.:5.652 3rd Qu.:1.0986 3rd Qu.:1.6094 3rd Qu.: 7.000 ## Max. :7.772 Max. :3.3322 Max. :2.3026 Max. :10.000 ## ## Class FreqSingular FreqPlural DerivEntropy ## animal:924 Min. : 4.0 Min. : 0.0 Min. :0.0000 ## plant :735 1st Qu.: 23.0 1st Qu.: 19.0 1st Qu.:0.0000 ## Median : 69.0 Median : 49.0 Median :0.0370 ## Mean : 132.1 Mean :109.7 Mean :0.3856 ## 3rd Qu.: 146.0 3rd Qu.:132.0 3rd Qu.:0.6845 ## Max. :1518.0 Max. :854.0 Max. :2.2641 ## ## Complex rInfl meanRT SubjFreq ## complex: 210 Min. :-1.3437 Min. :6.245 Min. :2.000 ## simplex:1449 1st Qu.:-0.3023 1st Qu.:6.322 1st Qu.:3.160 ## Median : 0.1900 Median :6.364 Median :3.880 ## Mean : 0.2845 Mean :6.379 Mean :3.911 ## 3rd Qu.: 0.6385 3rd Qu.:6.420 3rd Qu.:4.680 ## Max. : 4.4427 Max. :6.621 Max. :6.040 ## ## meanSize meanWeight BNCw BNCc ## Min. :1.323 Min. :0.8244 Min. : 0.02229 Min. : 0.0000 ## 1st Qu.:1.890 1st Qu.:1.4590 1st Qu.: 1.64921 1st Qu.: 0.1625 ## Median :3.099 Median :2.7558 Median : 3.32071 Median : 0.6500 ## Mean :2.891 Mean :2.5516 Mean : 7.37800 Mean : 5.0351 ## 3rd Qu.:3.711 3rd Qu.:3.4178 3rd Qu.: 7.10943 3rd Qu.: 2.9248 ## Max. :4.819 Max. :4.7138 Max. :79.17324 Max. :83.1949 ## ## BNCd BNCcRatio BNCdRatio ## Min. : 0.000 Min. :0.00000 Min. :0.0000 ## 1st Qu.: 1.188 1st Qu.:0.09673 1st Qu.:0.5551 ## Median : 3.800 Median :0.27341 Median :0.9349 ## Mean : 12.995 Mean :0.45834 Mean :1.5428 ## 3rd Qu.: 10.451 3rd Qu.:0.55550 3rd Qu.:2.1315 ## Max. :241.561 Max. :8.29545 Max. :6.3458 ## summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 4.2 Continous variable A continuous variable can take any of an infinite set of ordered values. You can use a histogram and other descriptive stats to characterize its distribution. A histogram is a type of graph in which each column represents a numeric variable, in particular that which is continuous and/or grouped.It shows the distribution of all observations in a quantitative dataset. It is useful for describing the shape, centre and spread to better understand the distribution of the dataset. How to interprate: The height of the column shows the frequency for a specific range of values. Columns are usually of equal width, however a histogram may show data using unequal ranges (intervals) and therefore have columns of unequal width. The values represented by each column must be mutually exclusive and exhaustive. Therefore, there are no spaces between columns and each observation can only ever belong in one column. It is important that there is no ambiguity in the labelling of the intervals on the x-axis for continuous or grouped data (e.g. 0 to less than 10, 10 to less than 20, 20 to less than 30). # histogram ggplot(diamonds)+ geom_histogram(aes(carat), binwidth = 0.1) # histogram zoom in y axsis ggplot(diamonds)+ geom_histogram(aes(carat), binwidth = 0.1)+ coord_cartesian(ylim = c(0,50)) # histogram zoom in x axsis ggplot(diamonds[ which(diamonds$carat &lt; 3), ])+ geom_histogram(aes(carat), binwidth = 0.1) diamonds%&gt;% filter(carat &lt;3)%&gt;% ggplot()+ geom_histogram(aes(carat), binwidth = 0.1) # density plot ggplot(diamonds,aes(carat))+ geom_density(kernel = &quot;gaussian&quot;) # area ggplot(diamonds,aes(carat))+ geom_area(stat = &quot;bin&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # dotplot ggplot(diamonds,aes(carat))+ geom_dotplot() ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. # freqpoly ggplot(diamonds, aes(carat))+ geom_freqpoly() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 4.2.1 Five-number summary The five-number summary is a set of descriptive statistics that provide information about a dataset. It consists of the five most important sample percentiles: the sample minimum (smallest observation) the lower quartile or first quartile the median (the middle value) the upper quartile or third quartile the sample maximum (largest observation) The five-number summary provides a concise summary of the distribution of the observations. Reporting five numbers avoids the need to decide on the most appropriate summary statistic. The five-number summary gives information about the location (from the median), spread (from the quartiles) and range (from the sample minimum and maximum) of the observations. Since it reports order statistics (rather than, say, the mean) the five-number summary is appropriate for ordinal measurements, as well as interval and ratio measurements. (from wikipedia) How to Find a Five-Number Summary: Step 1: Put your numbers in ascending order (from smallest to largest). For this particular data set, the order is: &gt; Example: 1,2,5,6,7,9,12,15,18,19,27. Step 2: Find the minimum and maximum for your data set. Now that your numbers are in order, this should be easy to spot. In the example in step 1, the minimum (the smallest number) is 1 and the maximum (the largest number) is 27. Step 3: Find the median. The median is the middle number. If you aren’t sure how to find the median, see: How to find the mean mode and median. Step 4: Place parentheses around the numbers above and below the median.(This is not technically necessary, but it makes Q1 and Q3 easier to find). (1,2,5,6,7),9,(12,15,18,19,27). Step 5: Find Q1 and Q3. Q1 can be thought of as a median in the lower half of the data, and Q3 can be thought of as a median for the upper half of data. (1,2,5,6,7), 9, ( 12,15,18,19,27). -Step 6: Write down your summary found in the above steps. minimum=1, Q1 =5, median=9, Q3=18, and maximum=27. summary(diamonds$carat) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.2000 0.4000 0.7000 0.7979 1.0400 5.0100 4.2.2 Boxplot &amp; violin plot It is possible to quickly compare several sets of observations by comparing their five-number summaries, which can be represented graphically using a boxplot. In descriptive statistics, a boxplot is a method for graphically depicting groups of numerical data through their quartiles. Boxplots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points. The spacings between the different parts of the box indicate the degree of dispersion (spread) and skewness in the data, and show outliers. Box plots can be drawn either horizontally or vertically. Box plots received their name from the box in the middle. box plots typically graph six data points: The lowest value, excluding outliers The first quartile (this is the 25th percentile, or median of all the numbers below the median) The median value (equivalent to the 50th percentile) The third quartile (this is the 75th percentile, or median of all the numbers above the median) The highest value, excluding outliers Outliers knitr::include_graphics(&quot;img/whatsaboxplot.png&quot;) Figure 4.1: Boxplot #source:https://mode.com/blog/how-to-make-box-and-whisker-plot-sql # r base function boxplot(diamonds$carat) #ggplot version ggplot(diamonds, aes(x=&quot;carat&quot;, y=carat))+ geom_boxplot() A violin plot is a method of plotting numeric data. It is similar to a boxplot, with the addition of a rotated kernel density plot on each side. Violin plots are similar to boxplots. Typically a violin plot will include all the data that is in a box plot: a marker for the median of the data; a box or marker indicating the interquartile range; and possibly all sample points, if the number of samples is not too high. knitr::include_graphics(&quot;img/violinplot.png&quot;) Figure 4.2: Violin plot #source:https://mode.com/blog/violin-plot-examples So a violin plot is more informative than a plain box plot. While a box plot only shows summary statistics such as mean/median and interquartile ranges, the violin plot shows the full distribution of the data. The difference is particularly useful when the data distribution is multimodal (more than one peak). In this case a violin plot shows the presence of different peaks, their position and relative amplitude. #violin plot ggplot(diamonds, aes(x=&quot;carat&quot;, y=carat))+ geom_violin() ggplot(diamonds, aes(x=&quot;price&quot;, y=price))+ geom_violin() 4.2.3 QQ Plots QQ Plots (Quantile-Quantile plots) are plots of two quantiles against each other. A quantile is a fraction where certain values fall below that quantile. For example, the median is a quantile where 50% of the data fall below that point and 50% lie above it. The purpose of QQ plots is to find out if two sets of data come from the same distribution. A 45 degree angle is plotted on the Q Q plot; if the two data sets come from a common distribution, the points will fall on that reference line.(from Wikipedia) # Solution 1 qplot(sample = carat, data = diamonds) # solution 2 ggplot(diamonds)+ geom_qq(aes(sample = carat))+ geom_qq_line(aes(sample = carat)) ### The measures of central tendency? (ABS) A measure of central tendency (also referred to as measures of centre or central location) is a summary measure that attempts to describe a whole set of data with a single value that represents the middle or centre of its distribution. There are three main measures of central tendency: the mode, the median and the mean. Each of these measures describes a different indication of the typical or central value in the distribution. The mode is the most commonly occurring value in a distribution. Consider this dataset showing the retirement age of 11 people, in whole years: 54, 54, 54, 55, 56, 57, 57, 58, 58, 60, 60 Advantage of the mode: The mode has an advantage over the median and the mean as it can be found for both numerical and categorical (non-numerical) data. Limitations of the mode: In some distributions, the mode may not reflect the centre of the distribution very well. it is easy to see that the centre of the distribution is 57 years, but the mode is lower, at 54 years. 54, 54, 54, 55, 56, 57, 57, 58, 58, 60, 60 more than one mode for the same distribution of data, (bi-modal, or multi-modal). The presence of more than one mode can limit the ability of the mode in describing the centre or typical value of the distribution because a single value to describe the centre cannot be identified. In some cases, particularly where the data are continuous, the distribution may have no mode at all (i.e. if all values are different).In cases such as these, it may be better to consider using the median or mean, or group the data in to appropriate intervals, and find the modal class. The median is the middle value in distribution when the values are arranged in ascending or descending order. The median divides the distribution in half (there are 50% of observations on either side of the median value). In a distribution with an odd number of observations, the median value is the middle value. 54, 54, 54, 55, 56, 57, 57, 58, 58, 60, 60 When the distribution has an even number of observations, the median value is the mean of the two middle values. In the following distribution, the two middle values are 56 and 57, therefore the median equals 56.5 years: 52, 54, 54, 54, 55, 56, 57, 57, 58, 58, 60, 60 Advantage of the median: The median is less affected by outliers and skewed data than the mean, and is usually the preferred measure of central tendency when the distribution is not symmetrical. Limitation of the median: The median cannot be identified for categorical nominal data, as it cannot be logically ordered. The mean is the sum of the value of each observation in a dataset divided by the number of observations. This is also known as the arithmetic average. Looking at the retirement age distribution again: 54, 54, 54, 55, 56, 57, 57, 58, 58, 60, 60 The mean is 56.6 years. Advantage of the mean: The mean can be used for both continuous and discrete numeric data. Limitations of the mean: The mean cannot be calculated for categorical data, as the values cannot be summed. As the mean includes every value in the distribution the mean is influenced by outliers and skewed distributions. What else do I need to know about the mean? The population mean is indicated by the Greek symbol µ (pronounced ‘mu’). When the mean is calculated on a distribution from a sample it is indicated by the symbol x̅ (pronounced X-bar). 4.2.4 How does the shape of a distribution influence the Measures of Central Tendency? 4.2.4.1 Symmetrical distributions: When a distribution is symmetrical, the mode, median and mean are all in the middle of the distribution. The following graph shows a larger retirement age dataset with a distribution which is symmetrical. The mode, median and mean all equal 58 years. hist(rbeta(10000,5,2)) hist(rbeta(10000,2,5)) hist(rbeta(10000,5,5)) knitr::include_graphics(&quot;img/normal.jpg&quot;) Figure 4.3: distribution knitr::include_graphics(&quot;img/right.jpg&quot;) Figure 4.3: distribution knitr::include_graphics(&quot;img/left.jpg&quot;) Figure 4.3: distribution #source:https://www.abs.gov.au/websitedbs/a3121120.nsf/home/statistical+language+-+measures+of+central+tendency 4.2.4.2 Skewed distributions: When a distribution is skewed the mode remains the most commonly occurring value, the median remains the middle value in the distribution, but the mean is generally ‘pulled’ in the direction of the tails. In a skewed distribution, the median is often a preferred measure of central tendency, as the mean is not usually in the middle of the distribution. A distribution is said to be positively or right skewed when the tail on the right side of the distribution is longer than the left side. In a positively skewed distribution it is common for the mean to be ‘pulled’ toward the right tail of the distribution. Although there are exceptions to this rule, generally, most of the values, including the median value, tend to be less than the mean value. The following graph shows a larger retirement age data set with a distribution which is right skewed. The data has been grouped into classes, as the variable being measured (retirement age) is continuous. The mode is 54 years, the modal class is 54-56 years, the median is 56 years and the mean is 57.2 years. A distribution is said to be negatively or left skewed when the tail on the left side of the distribution is longer than the right side. In a negatively skewed distribution, it is common for the mean to be ‘pulled’ toward the left tail of the distribution. Although there are exceptions to this rule, generally, most of the values, including the median value, tend to be greater than the mean value. The following graph shows a larger retirement age dataset with a distribution which left skewed. The mode is 65 years, the modal class is 63-65 years, the median is 63 years and the mean is 61.8 years. 4.2.5 Measures of Spread Summarising the dataset can help us understand the data, especially when the dataset is large. As discussed in the Measures of Central Tendency page, the mode, median, and mean summarise the data into a single value that is typical or representative of all the values in the dataset, but this is only part of the ‘picture’ that summarises a dataset. Measures of spread summarise the data in a way that shows how scattered the values are and how much they differ from the mean value.(ABS) Measures of spread describe how similar or varied the set of observed values are for a particular variable (data item). Measures of spread include the range, quartiles and the interquartile range, variance and standard deviation.(ABS) The spread of the values can be measured for quantitative data, as the variables are numeric and can be arranged into a logical order with a low end value and a high end value.(ABS) 4, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 8 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11 the measures of central tendency for both datasets are the same.However, if we look at the spread of the values, we can see that Dataset B is more dispersed than Dataset A. Used together, the measures of central tendency and measures of spread help us to better understand the data dataset1 = c(4, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 8) dataset2 = c(1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11) dataset = data.frame(dataset1, dataset2) dataset = gather(dataset, type, data) ggplot(dataset, aes(data, fill = type)) + geom_bar(position=position_dodge()) The range is the difference between the smallest value and the largest value in a dataset. range(dataset1) ## [1] 4 8 range(dataset2) ## [1] 1 11 Quartiles divide an ordered dataset into four equal parts, and refer to the values of the point between the quarters. A dataset may also be divided into quintiles (five equal parts) or deciles (ten equal parts). The lower quartile (Q1) is the point between the lowest 25% of values and the highest 75% of values. It is also called the 25th percentile. The second quartile (Q2) is the middle of the data set. It is also called the 50th percentile, or the median. The upper quartile (Q3) is the point between the lowest 75% and highest 25% of values. It is also called the 75th percentile. The interquartile range (IQR) is the difference between the upper (Q3) and lower (Q1) quartiles, and describes the middle 50% of values when ordered from lowest to highest. The IQR is often seen as a better measure of spread than the range as it is not affected by outliers. quantile(dataset1) ## 0% 25% 50% 75% 100% ## 4 5 6 7 8 quantile(dataset2) ## 0% 25% 50% 75% 100% ## 1.00 3.75 6.00 8.25 11.00 The variance and the standard deviation are measures of the spread of the data around the mean. They summarise how close each observed data value is to the mean value. In datasets with a small spread all values are very close to the mean, resulting in a small variance and standard deviation. Where a dataset is more dispersed, values are spread further away from the mean, leading to a larger variance and standard deviation. The smaller the variance and standard deviation, the more the mean value is indicative of the whole dataset. Therefore, if all values of a dataset are the same, the standard deviation and variance are zero. The standard deviation of a normal distribution enables us to calculate confidence intervals. In a normal distribution, about 68% of the values are within one standard deviation either side of the mean and about 95% of the scores are within two standard deviations of the mean. The standard deviation is the square root of the variance. var(dataset1) ## [1] 1.272727 sqrt(var(dataset1)) ## [1] 1.128152 sd(dataset1) ## [1] 1.128152 Confidence interval (Taken from Summary and Analysis of Extension Program Evaluation in R by Salvatore S. Mangiafico) A confidence interval is a range in which it is estimated the true population value lies. A confidence interval is used to indicate how accurate a calculated statistic is likely to be. Confidence intervals can be calculated for a variety of statistics, such as the mean, median, or slope of a linear regression. This chapter will focus on confidences intervals for means. Most of the statistics we use assume we are analyzing a sample which we are using to represent a larger population. Statistics and parameters When we calculate the sample mean, the result is a statistic. It’s an estimate of the population mean, but our calculated sample mean would vary depending on our sample. In theory, there is a mean for the population of interest, and we consider this population mean a parameter. Our goal in calculating the sample mean is estimating the population parameter. Point estimates and confidence intervals Our sample mean is a point estimate for the population parameter. A point estimate is a useful approximation for the parameter, but considering the confidence interval for the estimate gives us more information. As a definition of confidence intervals, if we were to sample the same population many times and calculated a sample mean and a 95% confidence interval each time, then 95% of those intervals would contain the actual population mean. if we want to compare the means of two groups to see if they are statistically different, we will use a t-test, or similar test, calculate a p-value, and draw a conclusion. An alternative approach would be to construct 95% or 99% confidence intervals about the mean for each group. If the confidence intervals of the two means don’t overlap, we are justified in calling them statistically different. Confidence intervals for means can be calculated by various methods. The traditional method is the most commonly encountered, and is appropriate for normally distributed data or with large sample sizes. It produces an interval that is symmetric about the mean.For skewed data, confidence intervals by bootstrapping may be more reliable.For routine use, I recommend using bootstrapped confidence intervals, particularly the BCa or percentile methods. The groupwiseMean function in the rcompanion package can produce confidence intervals both by traditional and bootstrap methods, for grouped and ungrouped data. The data must be housed in a data frame. By default, the function reports confidence intervals by the traditional method. In the groupwiseMean function, the measurement and grouping variables can be indicated with formula notation, with the measurement variable on the left side of the tilde (~), and grouping variables on the right. The confidence level is indicated by, e.g., the conf = 0.95 argument. The digits option indicates the number of significant digits to which the output is rounded. Note that in the output, the means and other statistics are rounded to 3 significant figures. #install.packages(&quot;rcompanion&quot;) library(rcompanion) dataset3 = data.frame(dataset1) groupwiseMean(dataset1 ~ 1, data = dataset3, conf = 0.95, digits = 3) ## .id n Mean Conf.level Trad.lower Trad.upper ## 1 &lt;NA&gt; 12 6 0.95 5.28 6.72 groupwiseMean(data ~ type, data = dataset, conf = 0.95, digits = 3) ## type n Mean Conf.level Trad.lower Trad.upper ## 1 dataset1 12 6 0.95 5.28 6.72 ## 2 dataset2 12 6 0.95 3.99 8.01 Bootstrapped means by group In the groupwiseMean function, the type of confidence interval is requested by setting certain options to TRUE. These options are traditional, normal, basic, percentile and bca. The boot option reports an optional statistic, the mean by bootstrap. The R option indicates the number of iterations to calculate each bootstrap statistic. groupwiseMean(data ~ type, data = dataset, conf = 0.95, digits = 3, R = 10000, boot = TRUE, traditional = FALSE, normal = FALSE, basic = FALSE, percentile = FALSE, bca = TRUE) ## type n Mean Boot.mean Conf.level Bca.lower Bca.upper ## 1 dataset1 12 6 6.00 0.95 5.33 6.50 ## 2 dataset2 12 6 6.01 0.95 4.17 7.58 4.2.6 Unusual values unusual = diamonds %&gt;% filter(y &lt; 3 | y &gt; 20)%&gt;% arrange(y) unusual ## # A tibble: 9 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Very Good H VS2 63.3 53 5139 0 0 0 ## 2 1.14 Fair G VS1 57.5 67 6381 0 0 0 ## 3 1.56 Ideal G VS2 62.2 54 12800 0 0 0 ## 4 1.2 Premium D VVS1 62.1 59 15686 0 0 0 ## 5 2.25 Premium H SI2 62.8 59 18034 0 0 0 ## 6 0.71 Good F SI2 64.1 60 2130 0 0 0 ## 7 0.71 Good F SI2 64.1 60 2130 0 0 0 ## 8 0.51 Ideal E VS1 61.8 55 2075 5.15 31.8 5.12 ## 9 2 Premium H SI2 58.9 57 12210 8.09 58.9 8.06 # drop the entire row with the strange values diamonds_new = diamonds%&gt;% filter(between(y,3,20)) head(arrange(diamonds_new,y)) ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.2 Premium D VS2 62.3 60 367 3.73 3.68 2.31 ## 2 0.2 Premium F VS2 62.6 59 367 3.73 3.71 2.33 ## 3 0.2 Very Good E VS2 63.4 59 367 3.74 3.71 2.36 ## 4 0.2 Premium D VS2 61.7 60 367 3.77 3.72 2.31 ## 5 0.2 Ideal E VS2 62.2 57 367 3.76 3.73 2.33 ## 6 0.2 Premium E SI2 60.2 62 345 3.79 3.75 2.27 # replace the unusual values with missing values diamonds_replace = diamonds %&gt;% mutate(y2 = ifelse(y&lt;3 | y&gt; 20, NA, y)) head(arrange(diamonds_replace, y )) ## # A tibble: 6 x 11 ## carat cut color clarity depth table price x y z y2 ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Very Good H VS2 63.3 53 5139 0 0 0 NA ## 2 1.14 Fair G VS1 57.5 67 6381 0 0 0 NA ## 3 1.56 Ideal G VS2 62.2 54 12800 0 0 0 NA ## 4 1.2 Premium D VVS1 62.1 59 15686 0 0 0 NA ## 5 2.25 Premium H SI2 62.8 59 18034 0 0 0 NA ## 6 0.71 Good F SI2 64.1 60 2130 0 0 0 NA "],
["covariation.html", "Chapter 5 Covariation 5.1 Categorical + continuous variable 5.2 Two categorical variables 5.3 Two continuous variables", " Chapter 5 Covariation If variation describes the behaviors within a variable, covariation describes the behavior between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way. 5.1 Categorical + continuous variable library(tidyverse) ggplot(diamonds, aes(price))+ geom_freqpoly(binwidth = 500) ggplot(diamonds, aes(price))+ geom_freqpoly(aes(color = cut), binwidth = 500) # standardized count where the area under each frequency polygon is one ggplot(diamonds, aes(x = price, y = ..density..))+ geom_freqpoly(aes(color = cut), binwidth = 500) # boxplot ggplot(diamonds, aes(cut, price))+ geom_boxplot() model = model name displ = engine displacement, in litres year = year of manufacture cyl = number of cylinders trans = type of transmission drv =&gt; f = front-wheel drive, r = rear wheel drive, 4 = 4wd cty = city miles per gallon hwy = highway miles per gallon fl = fuel type class = “type” of car summary(mpg) ## manufacturer model displ year ## Length:234 Length:234 Min. :1.600 Min. :1999 ## Class :character Class :character 1st Qu.:2.400 1st Qu.:1999 ## Mode :character Mode :character Median :3.300 Median :2004 ## Mean :3.472 Mean :2004 ## 3rd Qu.:4.600 3rd Qu.:2008 ## Max. :7.000 Max. :2008 ## cyl trans drv cty ## Min. :4.000 Length:234 Length:234 Min. : 9.00 ## 1st Qu.:4.000 Class :character Class :character 1st Qu.:14.00 ## Median :6.000 Mode :character Mode :character Median :17.00 ## Mean :5.889 Mean :16.86 ## 3rd Qu.:8.000 3rd Qu.:19.00 ## Max. :8.000 Max. :35.00 ## hwy fl class ## Min. :12.00 Length:234 Length:234 ## 1st Qu.:18.00 Class :character Class :character ## Median :24.00 Mode :character Mode :character ## Mean :23.44 ## 3rd Qu.:27.00 ## Max. :44.00 ggplot(mpg)+ geom_boxplot(aes(x = reorder(class, hwy, FUN = median), y = hwy)) ggplot(mpg)+ geom_boxplot(aes(x = reorder(class, hwy, FUN = median), y = hwy))+ coord_flip() 5.2 Two categorical variables ggplot(diamonds)+ geom_count(aes(cut, color)) diamonds%&gt;% count(color, cut) ## # A tibble: 35 x 3 ## color cut n ## &lt;ord&gt; &lt;ord&gt; &lt;int&gt; ## 1 D Fair 163 ## 2 D Good 662 ## 3 D Very Good 1513 ## 4 D Premium 1603 ## 5 D Ideal 2834 ## 6 E Fair 224 ## 7 E Good 933 ## 8 E Very Good 2400 ## 9 E Premium 2337 ## 10 E Ideal 3903 ## # ... with 25 more rows diamonds%&gt;% count(color, cut)%&gt;% ggplot(aes(color, cut))+ geom_tile(aes(fill=n)) #install.packages(&quot;seriation&quot;) 5.3 Two continuous variables ggplot(diamonds)+ geom_point(aes(carat, price)) # add transparency ggplot(diamonds)+ geom_point(aes(carat, price), alpha = 1/100) # bin two variables ggplot(diamonds)+ geom_bin2d(aes(carat, price)) #install.packages(&quot;hexbin&quot;) ggplot(diamonds)+ geom_hex(aes(carat, price)) #bin one variable ggplot(diamonds,aes(carat, price))+ geom_boxplot(aes(group = cut_width(carat, 0.1))) ggplot(diamonds,aes(carat, price))+ geom_boxplot(aes(group = cut_width(carat, 0.5))) "],
["from-data-visualization-to-statistical-modelling.html", "Chapter 6 From data visualization to statistical modelling 6.1 Two continuous/numerical variables", " Chapter 6 From data visualization to statistical modelling Patterns in the data provide clues about relationship or covariation.Now that we know how to visualize the various relationships, we can proceed to know more about how to formally test the relationship. Statistical models are tools for extracting patterns out of data. Statistics represent a common method of presenting information helping us to understand what the data are telling us. Descriptive (or summary) statistics summarise the raw data and allow data users to interpret a dataset more easily.Descriptive statistics can describe the shape, centre and spread of a dataset. Inferential statistics are used to infer conclusions about a population from a sample of that population. Inferential statistics are the result of techniques that use the data collected from a sample to make generalisations about the whole population from which the sample was taken. Inferential statistics include estimation (An estimate is a value that is inferred for a population based on data collected from a sample of units from that population), and hypothesis testing. knitr::include_graphics(&quot;img/stats.png&quot;) Figure 6.1: stats #source:from slides, Dr.Russell Thomson, Statistical Consultant, Centre for Research in Mathematics and Graduate Research School 6.1 Two continuous/numerical variables The techique we used here is called Simpler linear regression, where there is one dependent variable (continuous) and one independent variable (continuous). When there are more than one independent variable (continuous), you need to look for something called Multiple linear regression. head(faithful) ## eruptions waiting ## 1 3.600 79 ## 2 1.800 54 ## 3 3.333 74 ## 4 2.283 62 ## 5 4.533 85 ## 6 2.883 55 ggplot(faithful)+ geom_point(aes(eruptions, waiting)) Correlation and linear regression each explore the relationship between two quantitative variables. (Salvatore S. Mangiafico) Correlation determines if one variable varies systematically as another variable changes. It does not specify that one variable is the dependent variable and the other is the independent variable. Often, it is useful to look at which variables are correlated to others in a data set, and it is especially useful to see which variables correlate to a particular variable of interest. In contrast, linear regression specifies one variable as the independent variable and another as the dependent variable. The resultant model relates the variables with a linear relationship. The tests associated with linear regression are parametric and assume normality, homoscedasticity, and independence of residuals, as well as a linear relationship between the two variables. if(!require(psych)){install.packages(&quot;psych&quot;)} ## Loading required package: psych ## ## Attaching package: &#39;psych&#39; ## The following object is masked from &#39;package:rcompanion&#39;: ## ## phi ## The following object is masked from &#39;package:Hmisc&#39;: ## ## describe ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## %+%, alpha if(!require(PerformanceAnalytics)){install.packages(&quot;PerformanceAnalytics&quot;)} ## Loading required package: PerformanceAnalytics ## Loading required package: xts ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## ## Attaching package: &#39;xts&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## first, last ## ## Attaching package: &#39;PerformanceAnalytics&#39; ## The following object is masked from &#39;package:graphics&#39;: ## ## legend if(!require(ggplot2)){install.packages(&quot;ggplot2&quot;)} if(!require(rcompanion)){install.packages(&quot;rcompanion&quot;)} library(psych) pairs(data=faithful, ~ eruptions + waiting) pairs(data=iris, ~ Sepal.Length + Sepal.Width + Petal.Length +Petal.Width) corr.test(faithful, use = &quot;pairwise&quot;, method = &quot;pearson&quot;, adjust = &quot;none&quot;) ## Call:corr.test(x = faithful, use = &quot;pairwise&quot;, method = &quot;pearson&quot;, ## adjust = &quot;none&quot;) ## Correlation matrix ## eruptions waiting ## eruptions 1.0 0.9 ## waiting 0.9 1.0 ## Sample Size ## [1] 272 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## eruptions waiting ## eruptions 0 0 ## waiting 0 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option library(PerformanceAnalytics) chart.Correlation(faithful, method=&quot;pearson&quot;, histogram=TRUE, pch=16) chart.Correlation(faithful, method=&quot;kendall&quot;, histogram=TRUE, pch=16) chart.Correlation(faithful, method=&quot;spearman&quot;, histogram=TRUE, pch=16) ## Warning in cor.test.default(as.numeric(x), as.numeric(y), method = method): ## Cannot compute exact p-value with ties The statistics r, rho, and tau are used as effect sizes for Pearson, Spearman, and Kendall regression, respectively. These statistics vary from –1 to 1, with 0 indicating no correlation, 1 indicating a perfect positive correlation, and –1 indicating a perfect negative correlation. Like other effect size statistics, these statistics are not affected by sample size. Interpretation of effect sizes necessarily varies by discipline and the expectations of the experiment. They should not be considered universal. An interpretation of r is given by Cohen (1988). It is probably reasonable to use similar interpretations for rho and tau. small: 0.10 – &lt; 0.30 medium: 0.30 – &lt; 0.50 large: ≥ 0.50 The test used for Pearson correlation is a parametric analysis that requires that the relationship between the variables is linear, and that the data be bivariate normal. Variables should be interval/ratio. The test is sensitive to outliers. The correlation coefficient, r, can range from +1 to –1, with +1 being a perfect positive correlation and –1 being a perfect negative correlation. An r of 0 represents no correlation whatsoever. The hypothesis test determines if the r value is significantly different from 0. Kendall correlation is considered a nonparametric analysis. It is a rank-based test that does not require assumptions about the distribution of the data. Variables can be interval/ratio or ordinal. The correlation coefficient from the test is tau, which can range from +1 to –1, with +1 being a perfect positive correlation and –1 being a perfect negative correlation. A tau of 0 represents no correlation whatsoever. The hypothesis test determines if the tau value is significantly different from 0. As a technical note, the cor.test function in R calculates tau-b, which handles ties in ranks well. The test is relatively robust to outliers in the data. The test is sometimes cited for being reliable when there are small number of samples or when there are many ties in ranks. Spearman correlation is considered a nonparametric analysis. It is a rank-based test that does not require assumptions about the distribution of the data. Variables can be interval/ratio or ordinal. The correlation coefficient from the test, rho, can range from +1 to –1, with +1 being a perfect positive correlation and –1 being a perfect negative correlation. A rho of 0 represents no correlation whatsoever. The hypothesis test determines if the rho value is significantly different from 0. Spearman correlation is probably most often used with ordinal data. It tests for a monotonic relationship between the variables. It is relatively robust to outliers in the data. cor.test( ~ eruptions + waiting, data=faithful, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: eruptions and waiting ## t = 34.089, df = 270, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.8756964 0.9210652 ## sample estimates: ## cor ## 0.9008112 # the results report the p-value for the hypothesis test as well as the r value, written as cor, 0.849. cor.test( ~ eruptions + waiting, data=faithful, method = &quot;kendall&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: eruptions and waiting ## z = 13.902, p-value &lt; 2.2e-16 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.5747674 cor.test( ~ eruptions + waiting, data=faithful, method = &quot;spearman&quot;) ## Warning in cor.test.default(x = c(3.6, 1.8, 3.333, 2.283, 4.533, 2.883, : ## Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: eruptions and waiting ## S = 744660, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.7779721 6.1.0.1 Linear regression Linear regression is a very common approach to model the relationship between two interval/ratio variables. The method assumes that there is a linear relationship between the dependent variable and the independent variable, and finds a best fit model for this relationship. Interpretation of coefficients The outcome of linear regression includes estimating the intercept and the slope of the linear model. Multiple, nominal, and ordinal independent variables If there are multiple independent variables of interval/ratio type in the model, then linear regression expands to multiple regression. The polynomial regression example in this chapter is a form of multiple regression. If the independent variable were of nominal type, then the linear regression would become a one-way analysis of variance. Handling independent variables of ordinal type can be complicated. Often they are treated as either nominal type or interval/ratio type, although there are drawbacks to each approach. Assumptions Linear regression assumes a linear relationship between the two variables, normality of the residuals, independence of the residuals, and homoscedasticity of residuals. Linear regression can be performed with the lm function, which was the same function we used for analysis of variance. model = lm(eruptions ~ waiting, data = faithful) summary(model) ## ## Call: ## lm(formula = eruptions ~ waiting, data = faithful) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.29917 -0.37689 0.03508 0.34909 1.19329 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.874016 0.160143 -11.70 &lt;2e-16 *** ## waiting 0.075628 0.002219 34.09 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4965 on 270 degrees of freedom ## Multiple R-squared: 0.8115, Adjusted R-squared: 0.8108 ## F-statistic: 1162 on 1 and 270 DF, p-value: &lt; 2.2e-16 plot(eruptions ~ waiting, data=faithful, pch=16, xlab = &quot;waiting&quot;, ylab = &quot;eruptions&quot;) abline(model, col = &quot;blue&quot;, lwd = 2) x = residuals(model) library(rcompanion) plotNormalHistogram(x) The summary function for lm model objects includes estimates for model parameters (intercept and slope), as well as an r-squared value for the model and p-value for the model. How to read the model? The model produces a coefficient for the intercept (-1.87) and a coefficient for the slope (0.07);Each coefficient comes with three other numbers: its standard error, a t-value, and a p-value. The p-value tells us whether the coefficient is significantly different from zero. If the coefficient for a predictor is zero, there is no relation at all between the predictor and the dependent variable, in which case it is worthless as a predictor. In order to ascertain whether a coefficient is significantly different from zero, and hence potentially useful, a two-tailed t-test is carried out, using the t-value and the associated degrees of freedom. The t-value itself is the value of the coefficient divided by its standard error. This standard error is a measure of how sure we are about the estimate of the coefficient. The smaller the standard error, the smaller the confidence interval around the estimate, the less likely it is that zero will be included in the acceptance region, and hence the smaller the probability that it might just as well be zero. The residual standard error is a measure of how unsuccessful the model is; it gauges the variability in the dependent variable that we can’t handle through the predictor variables. The better a model is, the smaller its residual standard error will be. The multiple R-squared equals 0.8115. This R-squared is the squared correlation coefficient, r2, which quantifies, on a scale from 0 to 1, the proportion of the variance that the model explains. 6.1.1 T-test/ANOVA: categorical (independent variable )+ continuous (dependent variable) The two-sample unpaired t-test is a commonly used test that compares the means of two samples. • Data for each population are normally distributed • For Student’s t-test, the two samples need to have the same variance. However, Welch’s t-test, which is used by default in R, does not assume equal variances. • Observations between groups are independent. That is, not paired or repeated measures data • Moderate skewness is permissible if the data distribution is unimodal without outliers Hypotheses • Null hypothesis: The means of the populations from which the data were sampled for each group are equal. • Alternative hypothesis (two-sided): The means of the populations from which the data were sampled for each group are not equal. Interpretation Reporting significant results as “Mean of variable Y for group A was different than that for group B.” is acceptable. # simulating the data id = seq(from = 1, to = 100) temp1 = c(&quot;f&quot;,&quot;m&quot;) gender = sample(temp1, size = 100, replace = TRUE) a = rnorm(100, mean = 80, sd = 4) b = rnorm(100, mean = 60, sd = 4) c = rnorm(100, mean = 85, sd = 4) d = rep(c(1,3,2,4), times = 25) df = data.frame(id = id, sex = gender, chinese = a, math = b, english = c, class = d) df$class = as.factor(df$class) # test if chinese is significantly different from #distribution tests summary(df$class) ## 1 2 3 4 ## 25 25 25 25 #visualization plot(density(df$chinese)) qqnorm(df$chinese) #formal tests #null hypothesis: the distribution is normal shapiro.test(df$chinese) ## ## Shapiro-Wilk normality test ## ## data: df$chinese ## W = 0.98601, p-value = 0.3743 ks.test(df$chinese, &quot;pnorm&quot;, mean(df$chinese), sd(df$chinese)) ## ## One-sample Kolmogorov-Smirnov test ## ## data: df$chinese ## D = 0.060823, p-value = 0.8532 ## alternative hypothesis: two-sided #one sample t-test: one group and a fixed value mean(df$chinese) ## [1] 80.06681 t.test(df$chinese, mu = 78) ## ## One Sample t-test ## ## data: df$chinese ## t = 4.5855, df = 99, p-value = 1.326e-05 ## alternative hypothesis: true mean is not equal to 78 ## 95 percent confidence interval: ## 79.17247 80.96115 ## sample estimates: ## mean of x ## 80.06681 t.test(df$chinese, mu = 78, alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: df$chinese ## t = 4.5855, df = 99, p-value = 6.631e-06 ## alternative hypothesis: true mean is greater than 78 ## 95 percent confidence interval: ## 79.31842 Inf ## sample estimates: ## mean of x ## 80.06681 t.test(df$chinese, mu = 78, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: df$chinese ## t = 4.5855, df = 99, p-value = 1 ## alternative hypothesis: true mean is less than 78 ## 95 percent confidence interval: ## -Inf 80.8152 ## sample estimates: ## mean of x ## 80.06681 wilcox.test(df$chinese, mu = 78)# skewed distributions ## ## Wilcoxon signed rank test with continuity correction ## ## data: df$chinese ## V = 3674, p-value = 7.851e-05 ## alternative hypothesis: true location is not equal to 78 #probabilities of the things counted count = c(23, 45, 67,65,40,89,36) chisq.test(count) ## ## Chi-squared test for given probabilities ## ## data: count ## X-squared = 58.548, df = 6, p-value = 8.873e-11 #testing two groups #independent sample t-test v.s. paired sample t-test #independent sample t-test---compare means class1 = df[df$class == 1, ]$chinese class2 = df[df$class == 2, ]$chinese t.test(class1,class2) ## ## Welch Two Sample t-test ## ## data: class1 and class2 ## t = -0.30137, df = 47.989, p-value = 0.7644 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.147322 2.326821 ## sample estimates: ## mean of x mean of y ## 79.93500 80.34525 wilcox.test(class1,class2)#not normally distributed ## ## Wilcoxon rank sum test ## ## data: class1 and class2 ## W = 293, p-value = 0.7148 ## alternative hypothesis: true location shift is not equal to 0 var.test(class1,class2)#compare variance ## ## F test to compare two variances ## ## data: class1 and class2 ## F = 1.0311, num df = 24, denom df = 24, p-value = 0.9408 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.4543733 2.3398496 ## sample estimates: ## ratio of variances ## 1.031099 #paired sample t-test t.test(df$chinese, df$math, paired = T) ## ## Paired t-test ## ## data: df$chinese and df$math ## t = 34.207, df = 99, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 19.13178 21.48794 ## sample estimates: ## mean of the differences ## 20.30986 wilcox.test(df$chinese, df$math, paired = T) ## ## Wilcoxon signed rank test with continuity correction ## ## data: df$chinese and df$math ## V = 5050, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 #Analysis of Variance: ANOVA #Question: is there a difference between four classes? dif_class = aov (chinese ~ class, data = df) summary(dif_class) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## class 3 5.8 1.924 0.092 0.964 ## Residuals 96 2005.5 20.890 #How to report: there is a significant difference among different classes in terms of their Chinese scores, F (3, 96) = 0.87, p = .46. #the problem of multiple comparision #Bonferroni correction: a/n #Tukey&#39;s Honestly Significant difference: assuming the means for each level of the factor should be based on equal numbers of observation. #how is the difference like? TukeyHSD(dif_class) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = chinese ~ class, data = df) ## ## $class ## diff lwr upr p adj ## 2-1 0.4102504 -2.969808 3.790309 0.9888716 ## 3-1 0.3086552 -3.071403 3.688714 0.9951806 ## 4-1 -0.1916777 -3.571736 3.188381 0.9988301 ## 3-2 -0.1015952 -3.481654 3.278463 0.9998247 ## 4-2 -0.6019281 -3.981987 2.778130 0.9663714 ## 4-3 -0.5003329 -3.880392 2.879726 0.9801886 #diff= the difference in the means #lwr = the lower end points of the confidence interval # p adj = adjusted p value plot(TukeyHSD(dif_class)) #two-way ANOVA m1 &lt;- aov(chinese ~ sex + class, data = df) summary(m1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## sex 1 2.0 1.983 0.094 0.76 ## class 3 5.2 1.726 0.082 0.97 ## Residuals 95 2004.1 21.096 m2 &lt;- aov(chinese ~ sex * class, data = df) summary(m2) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## sex 1 2.0 1.983 0.091 0.763 ## class 3 5.2 1.726 0.080 0.971 ## sex:class 3 7.0 2.331 0.107 0.956 ## Residuals 92 1997.1 21.707 m3 &lt;- aov(chinese ~ sex + class + sex:class, data = df)# same as model2 summary(m3) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## sex 1 2.0 1.983 0.091 0.763 ## class 3 5.2 1.726 0.080 0.971 ## sex:class 3 7.0 2.331 0.107 0.956 ## Residuals 92 1997.1 21.707 TukeyHSD(m1) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = chinese ~ sex + class, data = df) ## ## $sex ## diff lwr upr p adj ## m-f 0.2853391 -1.562111 2.132789 0.7598017 ## ## $class ## diff lwr upr p adj ## 2-1 0.4559047 -2.941349 3.853159 0.9850777 ## 3-1 0.2972417 -3.100012 3.694496 0.9957505 ## 4-1 -0.1117827 -3.509037 3.285471 0.9997700 ## 3-2 -0.1586630 -3.555917 3.238591 0.9993443 ## 4-2 -0.5676874 -3.964941 2.829567 0.9719252 ## 4-3 -0.4090244 -3.806278 2.988230 0.9891243 TukeyHSD(m2) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = chinese ~ sex * class, data = df) ## ## $sex ## diff lwr upr p adj ## m-f 0.2853391 -1.589501 2.160179 0.7631285 ## ## $class ## diff lwr upr p adj ## 2-1 0.4559047 -2.992273 3.904082 0.9856822 ## 3-1 0.2972417 -3.150936 3.745419 0.9959256 ## 4-1 -0.1117827 -3.559960 3.336395 0.9997796 ## 3-2 -0.1586630 -3.606840 3.289514 0.9993716 ## 4-2 -0.5676874 -4.015865 2.880490 0.9730438 ## 4-3 -0.4090244 -3.857202 3.039153 0.9895675 ## ## $`sex:class` ## diff lwr upr p adj ## m:1-f:1 -0.42213946 -6.618277 5.773998 0.9999990 ## f:2-f:1 -0.07923486 -6.675527 6.517057 1.0000000 ## m:2-f:1 0.31005451 -6.183963 6.804072 0.9999999 ## f:3-f:1 0.12068786 -7.358804 7.600180 1.0000000 ## m:3-f:1 -0.01693359 -6.157747 6.123880 1.0000000 ## f:4-f:1 -0.89101777 -7.217959 5.435923 0.9998522 ## m:4-f:1 0.13969543 -6.715372 6.994763 1.0000000 ## f:2-m:1 0.34290461 -5.105940 5.791749 0.9999994 ## m:2-m:1 0.73219397 -4.592382 6.056770 0.9998741 ## f:3-m:1 0.54282732 -5.947291 7.032946 0.9999958 ## m:3-m:1 0.40520587 -4.482373 5.292785 0.9999960 ## f:4-m:1 -0.46887831 -5.588353 4.650596 0.9999921 ## m:4-m:1 0.56183490 -5.197574 6.321244 0.9999878 ## m:2-f:2 0.38928936 -5.396042 6.174620 0.9999991 ## f:3-f:2 0.19992271 -6.673256 7.073102 1.0000000 ## m:3-f:2 0.06230126 -5.323549 5.448151 1.0000000 ## f:4-f:2 -0.81178292 -6.408922 4.785357 0.9998200 ## m:4-f:2 0.21893029 -5.968940 6.406801 1.0000000 ## f:3-m:2 -0.18936665 -6.964453 6.585719 1.0000000 ## m:3-m:2 -0.32698810 -5.587081 4.933105 0.9999995 ## f:4-m:2 -1.20107228 -6.677309 4.275164 0.9973311 ## m:4-m:2 -0.17035907 -6.249088 5.908369 1.0000000 ## m:3-f:3 -0.13762145 -6.574943 6.299701 1.0000000 ## f:4-f:3 -1.01170563 -7.626818 5.603406 0.9997436 ## m:4-f:3 0.01900758 -7.102894 7.140909 1.0000000 ## f:4-m:3 -0.87408418 -5.926459 4.178291 0.9994209 ## m:4-m:3 0.15662903 -5.543219 5.856477 1.0000000 ## m:4-f:4 1.03071321 -4.869190 6.930616 0.9993826 "],
["statistical-modeling-and-model-interpretation.html", "Chapter 7 Statistical Modeling and Model Interpretation 7.1 Parametric statistical tests 7.2 formal tests for homogeneity of variance", " Chapter 7 Statistical Modeling and Model Interpretation 7.1 Parametric statistical tests T-test, analysis of variance, and linear regression are all parametric statistical tests. They are used when the dependent variable is an interval/ratio data variable, such as length, height, weight. Advantages: - your audience will likely be familiar with the techniques and interpretation of the results. - These tests are also often more flexible and more powerful than their nonparametric analogues. Drawback: - all parametric tests assume something about the distribution of the underlying data. If these assumptions are violated, the resultant test statistics will not be valid, and the tests will not be as powerful as for cases when assumptions are met. - Count data may not be appropriate for common parametric tests. Instead, count data could be analyzed either by using tests for nominal data or by using regression methods appropriate for count data, such as Poisson regression, negative binomial regression, and zero-inflated Poisson regression. Assumptions - Random sampling The data captured in the sample are randomly chosen from the population as a whole. Selection bias will obviously affect the validity of the outcome of the analysis. Independent observations Tests will also assume that observations are independent of one another, except when the analysis takes non-independence into account. For example, in repeated measures experiments, the same subject is observed over time. Students with a high test score on one date to have a high test score on subsequent dates. In this case the observation on one date would not be independent of observations on other dates.The independence of observation is often assumed from good experimental design. Also, data or residuals can be plotted, for example to see if observations from one date are correlated to those for another date. Normal distribution of data or residuals Parametric tests assume that the data come from a population of known distribution, such as normal distribution. That is, the data are normally distributed once the effects of the variables in the model are taken into account. Practically speaking, this means that the residuals from the analysis should be normally distributed. This will usually be assessed with a histogram of residuals, a density plot, or with quantile–quantile plot. A select number of tests (limited to one-sample t-test, two-sample t-test, and paired t-test) will require that data itself be normally distributed.For other tests, the distribution of the residuals will be investigated. Residuals from an analysis are also commonly called errors. They are the difference between the observations and the value predicted by the model. For example, if the calculated mean of a sample is 10, and one observation is 12, the residual for this observation is 2. If another observation is 7, the residual for this observation is –3. Be careful not to get confused about this assumption. You may see discussion about how “data” should be normally distributed for parametric tests. This is usually wrong-headed. The t-test assumes that the observations for each group are normally distributed, but if there is a difference in the groups, we might expect a bi-modal distribution, not a simple normal distribution, for the combined data. This is why in most cases we look at the distribution of the residuals, not the raw data. Assessing model assumptions Using formal tests to assess normality of residuals There are formal tests to assess the normality of residuals. Common tests include Shapiro-Wilk, Anderson–Darling, Kolmogorov–Smirnov, and D’Agostino–Pearson. However, their results are dependent on sample size. When the sample size is large, the tests may indicate a statistically significant departure from normality, even if that departure is small. And when sample sizes are small, they won’t detect departures from normality. In each case, the null hypothesis is that the data distribution is not different from normal. That is, a significant p-value (p &lt; 0.05) suggests that data are not normally distributed. library(tidyverse) head(diamonds) ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 summary(diamonds) ## carat cut color clarity ## Min. :0.2000 Fair : 1610 D: 6775 SI1 :13065 ## 1st Qu.:0.4000 Good : 4906 E: 9797 VS2 :12258 ## Median :0.7000 Very Good:12082 F: 9542 SI2 : 9194 ## Mean :0.7979 Premium :13791 G:11292 VS1 : 8171 ## 3rd Qu.:1.0400 Ideal :21551 H: 8304 VVS2 : 5066 ## Max. :5.0100 I: 5422 VVS1 : 3655 ## J: 2808 (Other): 2531 ## depth table price x ## Min. :43.00 Min. :43.00 Min. : 326 Min. : 0.000 ## 1st Qu.:61.00 1st Qu.:56.00 1st Qu.: 950 1st Qu.: 4.710 ## Median :61.80 Median :57.00 Median : 2401 Median : 5.700 ## Mean :61.75 Mean :57.46 Mean : 3933 Mean : 5.731 ## 3rd Qu.:62.50 3rd Qu.:59.00 3rd Qu.: 5324 3rd Qu.: 6.540 ## Max. :79.00 Max. :95.00 Max. :18823 Max. :10.740 ## ## y z ## Min. : 0.000 Min. : 0.000 ## 1st Qu.: 4.720 1st Qu.: 2.910 ## Median : 5.710 Median : 3.530 ## Mean : 5.735 Mean : 3.539 ## 3rd Qu.: 6.540 3rd Qu.: 4.040 ## Max. :58.900 Max. :31.800 ## test_data = diamonds%&gt;% filter(cut %in% c(&quot;Fair&quot;, &quot;Ideal&quot; ), carat == 0.7, color %in% c(&quot;G&quot;, &quot;F&quot; ), clarity %in% c(&quot;SI1&quot;, &quot;VS2&quot; )) table(test_data$cut) ## ## Fair Good Very Good Premium Ideal ## 21 0 0 0 113 ggplot(test_data, aes(price, fill = cut)) + geom_density(position=&quot;dodge&quot;, alpha = 0.6) ## Warning: Width not defined. Set with `position_dodge(width = ?)` #Define a linear model model = lm(price ~ cut + color, data = test_data) #Shapiro–Wilk normality test x = residuals(model) shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.96048, p-value = 0.0006383 # Anderson-Darling normality test if(!require(nortest)){install.packages(&quot;nortest&quot;)} ## Loading required package: nortest library(nortest) x = residuals(model) ad.test(x) ## ## Anderson-Darling normality test ## ## data: x ## A = 1.7123, p-value = 0.0002071 # One-sample Kolmogorov-Smirnov test x = residuals(model) ks.test(x, &quot;pnorm&quot;, mean = mean(x), sd = sd(x)) ## Warning in ks.test(x, &quot;pnorm&quot;, mean = mean(x), sd = sd(x)): ties should not ## be present for the Kolmogorov-Smirnov test ## ## One-sample Kolmogorov-Smirnov test ## ## data: x ## D = 0.096286, p-value = 0.1666 ## alternative hypothesis: two-sided # D&#39;Agostino Normality Test if(!require(fBasics)){install.packages(&quot;fBasics&quot;)} ## Loading required package: fBasics ## Loading required package: timeDate ## ## Attaching package: &#39;timeDate&#39; ## The following objects are masked from &#39;package:PerformanceAnalytics&#39;: ## ## kurtosis, skewness ## Loading required package: timeSeries ## ## Attaching package: &#39;timeSeries&#39; ## The following object is masked from &#39;package:zoo&#39;: ## ## time&lt;- ## The following object is masked from &#39;package:psych&#39;: ## ## outlier ## ## Attaching package: &#39;fBasics&#39; ## The following object is masked from &#39;package:psych&#39;: ## ## tr library(fBasics) x = residuals(model) dagoTest(x) ## ## Title: ## D&#39;Agostino Normality Test ## ## Test Results: ## STATISTIC: ## Chi2 | Omnibus: 10.2249 ## Z3 | Skewness: 3.1412 ## Z4 | Kurtosis: 0.5981 ## P VALUE: ## Omnibus Test: 0.006021 ## Skewness Test: 0.001683 ## Kurtosis Test: 0.5498 ## ## Description: ## Mon Oct 28 00:39:57 2019 by user: Juqiang Chen Skew and kurtosis There are no definitive guidelines as to what range of skew or kurtosis are acceptable for considering residuals to be normally distributed. In general, I would not recommend relying on skew and kurtosis calculations, but instead use histograms and other plots. If I were forced to give advice for skewness calculations, I might say, be cautious if the absolute value is &gt; 0.5, and consider it not normally distributed if the absolute value is &gt; 1.0. Some authors use 2.0 as a cutoff for normality, and others use a higher limit for kurtosis. library(psych) x = residuals(model) describe(x, type=2) ## vars n mean sd median trimmed mad min max range skew ## X1 1 134 0 284.79 -40.84 -20.5 262.42 -543.69 839.01 1382.7 0.7 ## kurtosis se ## X1 0.18 24.6 Using visual inspection to assess the normality of residuals Usually, the best method to see if model residuals meet the assumptions of normal distribution and homoscedasticity are to plot them and inspect the plots visually. Histogram with normal curve A histogram of the residuals should be approximately normal, without excessive skew or kurtosis. Adding a normal curve with the same mean and standard deviation as the data helps to assess the histogram. x = residuals(model) library(rcompanion) plotNormalHistogram(residuals(model)) Kernel density plot with normal curve A kernel density plot is similar to a histogram, but is smoothed into a curve. Sometimes a density plot gives a better representation of the distribution of data, because the appearance of the histogram depends upon how many bins are used. The plotNormalDensity function will produce this plot. Options include those for the plot function, as well as adjust, bw, and kernel which are passed to the density function. col1, col2, and col3 change plot colors, and lwd changes line thickness. library(rcompanion) x = residuals(model) plotNormalDensity(x, adjust = 1) ### Decrease this number ### to make line less smooth 7.2 formal tests for homogeneity of variance In each case, the null hypothesis is that the variance among groups is not different. That is, a significant p-value (p &lt; 0.05) suggests that the variance among groups is different. #Define a linear model model = lm(price ~ cut + color, data = test_data) #Bartlett’s test for homogeneity of variance #Bartlett’s test is known to be sensitive to non-normality in samples. That is, non-normal samples can result in a significant test due to the non-normality. x = residuals(model) bartlett.test(x ~ interaction(cut, color), data = test_data) ## ## Bartlett test of homogeneity of variances ## ## data: x by interaction(cut, color) ## Bartlett&#39;s K-squared = 39.068, df = 3, p-value = 1.679e-08 # Levene’s test for homogeneity of variance # Levene’s test is an alternative to Bartlett’s that is supposedly less sensitive to departures from normality in the data. if(!require(car)){install.packages(&quot;car&quot;)} ## Loading required package: car ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:fBasics&#39;: ## ## densityPlot ## The following object is masked from &#39;package:psych&#39;: ## ## logit ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following object is masked from &#39;package:purrr&#39;: ## ## some library(car) x = residuals(model) leveneTest(x ~ cut * color, data=test_data, center=mean) ### Use the original Levene’s test ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 3 8.76 2.468e-05 *** ## 130 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Brown–Forsythe or robust Levene’s test # The Brown–Forsythe modification of Levene’s test makes it more robust to departures in normality of the data. #if(!require(car)){install.packages(&quot;car&quot;)} library(car) x = residuals(model) leveneTest(x ~ cut * color, data=test_data) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 8.5541 3.163e-05 *** ## 130 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #if(!require(lawstat)){install.packages(&quot;lawstat&quot;)} library(lawstat) ## ## Attaching package: &#39;lawstat&#39; ## The following object is masked from &#39;package:car&#39;: ## ## levene.test x = residuals(model) levene.test(x, interaction(test_data$cut, test_data$color)) ## ## Modified robust Brown-Forsythe Levene-type test based on the ## absolute deviations from the median ## ## data: x ## Test Statistic = 0.51854, p-value = 0.6702 # Fligner-Killeen test # The Fligner-Killeen test is another test for homogeneity of variances that is robust to departures in normality of the data. x = residuals(model) fligner.test(x ~ interaction(cut, color), data=test_data) ## ## Fligner-Killeen test of homogeneity of variances ## ## data: x by interaction(cut, color) ## Fligner-Killeen:med chi-squared = 11.948, df = 3, p-value = ## 0.007562 "],
["resources.html", "Chapter 8 Resources", " Chapter 8 Resources "]
]
